{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from mcts import mcts\n",
    "\n",
    "from minigrid.core.grid import Grid\n",
    "from minigrid.core.mission import MissionSpace\n",
    "from minigrid.core.world_object import Goal\n",
    "from minigrid.manual_control import ManualControl\n",
    "from minigrid.minigrid_env import MiniGridEnv\n",
    "from gymnasium import spaces"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define the environment\n",
    "\n",
    "The GridWorld environment uses MiniGrid Gymnasium package, which provides a graphical interface\n",
    "\n",
    "**Parameters**\n",
    "\n",
    "- size (int): size of the grid\n",
    "\n",
    "- start_pos (int, int): starting position of the agent\n",
    "\n",
    "- start_dir (int): starting direction of the agent\n",
    "\n",
    "- goal_pos (int, int): starting position of the goal\n",
    "\n",
    "- agent_view_size (int): square radius of the agents view"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Environment class\n",
    "class GridWorld(MiniGridEnv):\n",
    "    def __init__(self, size=18, start_pos = None, start_dir = 0, goal_pos = None, **kwargs,):\n",
    "        self.agent_start_pos = start_pos\n",
    "        self.agent_start_dir = start_dir\n",
    "        self.goal_pos = goal_pos\n",
    "\n",
    "        mission_space = MissionSpace(mission_func=self._gen_mission)\n",
    "\n",
    "        super().__init__(\n",
    "            mission_space=mission_space,\n",
    "            grid_size=size,\n",
    "            **kwargs,\n",
    "        )\n",
    "\n",
    "    @staticmethod\n",
    "    def _gen_mission():\n",
    "        return 'Reach the goal'\n",
    "    \n",
    "    def _gen_grid(self, width, height):\n",
    "        #Create an empty grid\n",
    "        self.grid = Grid(width, height)\n",
    "\n",
    "        #Surround the grid with walls\n",
    "        self.grid.wall_rect(0, 0, width, height)\n",
    "\n",
    "        #Place the agent\n",
    "        if self.agent_start_pos is not None:\n",
    "            self.agent_pos = self.agent_start_pos\n",
    "            self.agent_dir = self.agent_start_dir\n",
    "        else:\n",
    "            self.place_agent()\n",
    "        \n",
    "        #Place the initial goal\n",
    "        if self.goal_pos is not None:\n",
    "            self.put_obj(Goal(), *self.goal_pos)\n",
    "        else:\n",
    "            self.place_obj(Goal())\n",
    "\n",
    "        self.mission = \"Reach the goal\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create the environment and get a test run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = GridWorld(render_mode = \"human\", size = 18, agent_view_size = 5)\n",
    "\n",
    "obs = env.reset()\n",
    "\n",
    "#Image returns the partial observation of the agent\n",
    "obs[0]['image']\n",
    "\n",
    "#Show the grid\n",
    "env.render()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MCTS-Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from copy import deepcopy\n",
    "\n",
    "#State class that represents the state of the agent\n",
    "class State:\n",
    "    def __init__(self, env, agent_pos, agent_dir, goal_belief=None):\n",
    "        self.env = deepcopy(env)\n",
    "        self.agent_pos = agent_pos\n",
    "        self.agent_dir = agent_dir\n",
    "        self.goal_belief = goal_belief if goal_belief is not None else self.initialize_goal_belief()\n",
    "\n",
    "    def initialize_goal_belief(self):\n",
    "        width, height = self.env.width - 1, self.env.height - 1\n",
    "        #Uniform distribution over all possible goal locations\n",
    "        #The goal can only be inside the walls\n",
    "        return np.ones((width, height)) / (width * height)\n",
    "\n",
    "    def update_belief(self, observation):\n",
    "        visible_area = observation[0]['image']\n",
    "        for i in range(visible_area.shape[0]):\n",
    "            for j in range(visible_area.shape[1]):\n",
    "                cell = visible_area[i, j]\n",
    "                #The type of the object\n",
    "                cell_type = cell[0]\n",
    "                if cell_type == 2:  # Goal\n",
    "                    self.belief[i, j] = 1.0  # Belief of goal location\n",
    "                else:\n",
    "                    self.belief[i, j] = 0.0  # Update based on observation\n",
    "        self.belief /= np.sum(self.belief)  # Normalize the belief state\n",
    "\n",
    "    def getPossibleActions(self):\n",
    "        return list(self.env.action_space)\n",
    "\n",
    "    def takeAction(self, action):\n",
    "        new_env = deepcopy(self.env)\n",
    "        new_env.agent_pos = self.agent_pos\n",
    "        new_env.agent_dir = self.agent_dir\n",
    "        obs, reward, done, _, _ = new_env.step(action)\n",
    "        new_state = State(new_env, new_env.agent_pos, new_env.agent_dir, deepcopy(self.belief))\n",
    "        new_state.update_belief(obs)\n",
    "        return new_state, reward, done\n",
    "\n",
    "    def isTerminal(self):\n",
    "        return self.env.grid.get(*self.agent_pos).type == 'goal'\n",
    "\n",
    "    def getReward(self):\n",
    "        if self.isTerminal():\n",
    "            return 0  # No steps remaining if goal is reached\n",
    "        return -1  # Negative reward for each step to encourage reaching the goal quickly\n",
    "\n",
    "    def __hash__(self):\n",
    "        return hash((self.agent_pos, self.agent_dir, self.belief.tobytes()))\n",
    "\n",
    "    def __eq__(self, other):\n",
    "        return self.agent_pos == other.agent_pos and self.agent_dir == other.agent_dir and np.array_equal(self.belief, other.belief)\n",
    "\n",
    "#MCTS Agent that makes use of the mcts library\n",
    "class POMDPMCTSAgent():\n",
    "    def __init__(self):\n",
    "        \n",
    "    def getAction(self, state):\n",
    "        pass\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'type'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m env \u001b[38;5;241m=\u001b[39m GridWorld(size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m10\u001b[39m, agent_view_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m3\u001b[39m)\n\u001b[1;32m      2\u001b[0m agent \u001b[38;5;241m=\u001b[39m Agent(env, visibility_range\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m3\u001b[39m)\n\u001b[0;32m----> 3\u001b[0m \u001b[43magent\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43mepisodes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[10], line 73\u001b[0m, in \u001b[0;36mAgent.run\u001b[0;34m(self, episodes)\u001b[0m\n\u001b[1;32m     71\u001b[0m initial_state \u001b[38;5;241m=\u001b[39m State(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39menv, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39menv\u001b[38;5;241m.\u001b[39magent_pos, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39menv\u001b[38;5;241m.\u001b[39magent_dir)\n\u001b[1;32m     72\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m done:\n\u001b[0;32m---> 73\u001b[0m     action \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_select_action\u001b[49m\u001b[43m(\u001b[49m\u001b[43minitial_state\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     74\u001b[0m     obs, reward, done, _, _ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39menv\u001b[38;5;241m.\u001b[39mstep(action)\n\u001b[1;32m     75\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m done:\n",
      "Cell \u001b[0;32mIn[10], line 64\u001b[0m, in \u001b[0;36mAgent._select_action\u001b[0;34m(self, state)\u001b[0m\n\u001b[1;32m     62\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_select_action\u001b[39m(\u001b[38;5;28mself\u001b[39m, state):\n\u001b[1;32m     63\u001b[0m     mcts_instance \u001b[38;5;241m=\u001b[39m mcts(timeLimit\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1000\u001b[39m)\n\u001b[0;32m---> 64\u001b[0m     best_action \u001b[38;5;241m=\u001b[39m \u001b[43mmcts_instance\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msearch\u001b[49m\u001b[43m(\u001b[49m\u001b[43minitialState\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstate\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     65\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m best_action\n",
      "File \u001b[0;32m~/anaconda3/envs/pml/lib/python3.10/site-packages/mcts.py:50\u001b[0m, in \u001b[0;36mmcts.search\u001b[0;34m(self, initialState)\u001b[0m\n\u001b[1;32m     49\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21msearch\u001b[39m(\u001b[38;5;28mself\u001b[39m, initialState):\n\u001b[0;32m---> 50\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mroot \u001b[38;5;241m=\u001b[39m \u001b[43mtreeNode\u001b[49m\u001b[43m(\u001b[49m\u001b[43minitialState\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m     52\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlimitType \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtime\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[1;32m     53\u001b[0m         timeLimit \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime() \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtimeLimit \u001b[38;5;241m/\u001b[39m \u001b[38;5;241m1000\u001b[39m\n",
      "File \u001b[0;32m~/anaconda3/envs/pml/lib/python3.10/site-packages/mcts.py:21\u001b[0m, in \u001b[0;36mtreeNode.__init__\u001b[0;34m(self, state, parent)\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, state, parent):\n\u001b[1;32m     20\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate \u001b[38;5;241m=\u001b[39m state\n\u001b[0;32m---> 21\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39misTerminal \u001b[38;5;241m=\u001b[39m \u001b[43mstate\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43misTerminal\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     22\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39misFullyExpanded \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39misTerminal\n\u001b[1;32m     23\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mparent \u001b[38;5;241m=\u001b[39m parent\n",
      "Cell \u001b[0;32mIn[10], line 43\u001b[0m, in \u001b[0;36mState.isTerminal\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     42\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21misTerminal\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m---> 43\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgrid\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43magent_pos\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtype\u001b[49m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mgoal\u001b[39m\u001b[38;5;124m'\u001b[39m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'type'"
     ]
    }
   ],
   "source": [
    "env = GridWorld(size=10, agent_view_size=3)\n",
    "agent = Agent(env, visibility_range=3)\n",
    "agent.run(episodes=1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
