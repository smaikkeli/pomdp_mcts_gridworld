{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "\n",
    "USE_CUDA = torch.cuda.is_available()\n",
    "device = torch.device(\"cuda\") if USE_CUDA else torch.device(\"cpu\")\n",
    "training = True\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO\n",
    "\n",
    "- Sample (state, action)-pairs. Split them to context and target sets!\n",
    "- Figure out how to split them correctly, what differences are there brtween\n",
    "- Train a neural process using the HIIT library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pygame 2.5.2 (SDL 2.28.2, Python 3.10.14)\n",
      "Hello from the pygame community. https://www.pygame.org/contribute.html\n"
     ]
    }
   ],
   "source": [
    "from src.utils.sampler import Sampler\n",
    "\n",
    "GRID_SIZE = 10\n",
    "AGENT_VIEW_SIZE = 3\n",
    "TRAJ_LENGTH = 10\n",
    "\n",
    "\n",
    "sampler = Sampler(grid_size = 10, agent_view_size = 5, traj_length = 10)\n",
    "batch = sampler.generate_batch([0.4,0.5])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of xc: torch.Size([1, 9, 2]), yc: torch.Size([1, 9, 5]), xt: torch.Size([1, 1, 2]), yt: torch.Size([1, 1, 5])\n",
      "Device of xc: cuda:0, yc: cuda:0, xt: cuda:0, yt: cuda:0\n"
     ]
    }
   ],
   "source": [
    "print(f\"Shape of xc: {batch[0].shape}, yc: {batch[1].shape}, xt: {batch[2].shape}, yt: {batch[3].shape}\")\n",
    "print(f\"Device of xc: {batch[0].device}, yc: {batch[1].device}, xt: {batch[2].device}, yt: {batch[3].device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[list([((5, 5), [1, 0, 0, 0, 0]), ((4, 5), [1, 0, 0, 0, 0]), ((3, 5), [1, 0, 0, 0, 0]), ((2, 5), [0, 0, 1, 0, 0]), ((2, 4), [0, 0, 1, 0, 0]), ((2, 3), [0, 0, 1, 0, 0]), ((2, 2), [1, 0, 0, 0, 0]), ((1, 2), [0, 1, 0, 0, 0]), ((2, 2), [0, 1, 0, 0, 0]), ((3, 2), [0, 1, 0, 0, 0]), ((4, 2), [0, 1, 0, 0, 0]), ((5, 2), [0, 1, 0, 0, 0]), ((6, 2), [0, 1, 0, 0, 0]), ((7, 2), [0, 0, 0, 1, 0]), ((7, 3), [0, 0, 0, 1, 0]), ((7, 4), [0, 0, 1, 0, 0]), ((7, 3), [0, 0, 0, 1, 0]), ((7, 4), [0, 0, 0, 1, 0]), ((7, 5), [0, 0, 0, 1, 0]), ((7, 5), [0, 0, 0, 0, 1])])\n",
      " list([((2, 3), [1, 0, 0, 0, 0]), ((1, 3), [0, 0, 0, 1, 0]), ((1, 4), [0, 0, 0, 1, 0]), ((1, 5), [0, 1, 0, 0, 0]), ((2, 5), [0, 1, 0, 0, 0]), ((3, 5), [0, 1, 0, 0, 0]), ((4, 5), [0, 1, 0, 0, 0]), ((5, 5), [0, 1, 0, 0, 0]), ((6, 5), [0, 1, 0, 0, 0]), ((7, 5), [0, 0, 1, 0, 0]), ((7, 4), [0, 0, 1, 0, 0]), ((7, 3), [1, 0, 0, 0, 0]), ((6, 3), [0, 1, 0, 0, 0]), ((7, 3), [0, 0, 1, 0, 0]), ((7, 2), [1, 0, 0, 0, 0]), ((6, 2), [1, 0, 0, 0, 0]), ((5, 2), [0, 0, 1, 0, 0]), ((5, 1), [1, 0, 0, 0, 0]), ((4, 1), [1, 0, 0, 0, 0]), ((3, 1), [1, 0, 0, 0, 0]), ((2, 1), [1, 0, 0, 0, 0])])\n",
      " list([((7, 5), [1, 0, 0, 0, 0]), ((6, 5), [1, 0, 0, 0, 0]), ((5, 5), [1, 0, 0, 0, 0]), ((4, 5), [1, 0, 0, 0, 0]), ((3, 5), [1, 0, 0, 0, 0]), ((2, 5), [0, 0, 0, 1, 0]), ((2, 6), [0, 0, 0, 1, 0]), ((2, 6), [0, 0, 0, 0, 1]), ((2, 6), [0, 0, 0, 0, 1]), ((2, 6), [0, 0, 0, 0, 1]), ((2, 6), [0, 0, 0, 0, 1]), ((2, 6), [0, 0, 0, 0, 1]), ((2, 6), [0, 0, 0, 0, 1]), ((2, 6), [0, 0, 0, 0, 1]), ((2, 6), [0, 0, 0, 0, 1]), ((2, 6), [0, 0, 0, 0, 1]), ((2, 6), [0, 0, 0, 0, 1]), ((2, 6), [0, 0, 0, 0, 1]), ((2, 6), [0, 0, 0, 0, 1]), ((2, 6), [0, 0, 0, 0, 1])])\n",
      " list([((3, 5), [1, 0, 0, 0, 0]), ((2, 5), [0, 0, 1, 0, 0]), ((2, 4), [0, 1, 0, 0, 0]), ((3, 4), [0, 0, 1, 0, 0]), ((3, 3), [0, 0, 1, 0, 0]), ((3, 3), [0, 0, 0, 0, 1]), ((3, 3), [0, 0, 0, 0, 1]), ((3, 3), [0, 0, 0, 0, 1]), ((3, 3), [0, 0, 0, 0, 1]), ((3, 3), [0, 0, 0, 0, 1]), ((3, 3), [0, 0, 0, 0, 1]), ((3, 3), [0, 0, 0, 0, 1]), ((3, 3), [0, 0, 0, 0, 1]), ((3, 3), [0, 0, 0, 0, 1]), ((3, 3), [0, 0, 0, 0, 1]), ((3, 3), [0, 0, 0, 0, 1]), ((3, 3), [0, 0, 0, 0, 1]), ((3, 3), [0, 0, 0, 0, 1]), ((3, 3), [0, 0, 0, 0, 1]), ((3, 3), [0, 0, 0, 0, 1])])\n",
      " list([((1, 4), [0, 0, 1, 0, 0]), ((1, 3), [0, 0, 0, 1, 0]), ((1, 4), [0, 0, 0, 1, 0]), ((1, 5), [0, 1, 0, 0, 0]), ((2, 5), [0, 1, 0, 0, 0]), ((3, 5), [0, 1, 0, 0, 0]), ((4, 5), [0, 1, 0, 0, 0]), ((5, 5), [0, 1, 0, 0, 0]), ((6, 5), [0, 0, 1, 0, 0]), ((6, 4), [0, 1, 0, 0, 0]), ((7, 4), [0, 1, 0, 0, 0]), ((8, 4), [0, 0, 0, 1, 0]), ((8, 5), [0, 0, 0, 1, 0]), ((8, 5), [0, 0, 0, 0, 1]), ((8, 5), [0, 0, 0, 0, 1]), ((8, 5), [0, 0, 0, 0, 1]), ((8, 5), [0, 0, 0, 0, 1]), ((8, 5), [0, 0, 0, 0, 1]), ((8, 5), [0, 0, 0, 0, 1]), ((8, 5), [0, 0, 0, 0, 1])])]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_22293/2587577198.py:10: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  print(np.array(traj))\n"
     ]
    }
   ],
   "source": [
    "from src.utils.sampler import Sampler\n",
    "\n",
    "sampler = Sampler(grid_size = 8, agent_view_size = 3, traj_length = 20)\n",
    "\n",
    "import numpy as np\n",
    "user_params = sampler.generate_user_parameters()\n",
    "\n",
    "traj = sampler.generate_user_trajectories(5, user_params)\n",
    "\n",
    "print(np.array(traj))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conditional Neural Process\n",
    "\n",
    "CNPs take in pairs (x, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training loop "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing the library\n",
    "\n",
    "Change: Use cross entropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean loss 4.375207979079614\n",
      "mean loss 4.308122355222105\n",
      "mean loss 2.0004657951909284\n",
      "mean loss 0.9779666955962762\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[53], line 26\u001b[0m\n\u001b[1;32m     24\u001b[0m loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m-\u001b[39mtorch\u001b[38;5;241m.\u001b[39mmean(nps\u001b[38;5;241m.\u001b[39mloglik(cnp, xc, yc, xt, yt))\n\u001b[1;32m     25\u001b[0m opt\u001b[38;5;241m.\u001b[39mzero_grad(set_to_none\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m---> 26\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     27\u001b[0m opt\u001b[38;5;241m.\u001b[39mstep()\n\u001b[1;32m     29\u001b[0m total_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m loss\u001b[38;5;241m.\u001b[39mitem()\n",
      "File \u001b[0;32m~/anaconda3/envs/pml2/lib/python3.10/site-packages/torch/_tensor.py:525\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    515\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    517\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    518\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    523\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    524\u001b[0m     )\n\u001b[0;32m--> 525\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    526\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    527\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/pml2/lib/python3.10/site-packages/torch/autograd/__init__.py:267\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    262\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    264\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[1;32m    265\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    266\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 267\u001b[0m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    268\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    269\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    270\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    271\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    272\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    273\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    274\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    275\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/pml2/lib/python3.10/site-packages/torch/autograd/graph.py:744\u001b[0m, in \u001b[0;36m_engine_run_backward\u001b[0;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    742\u001b[0m     unregister_hooks \u001b[38;5;241m=\u001b[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[1;32m    743\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 744\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    745\u001b[0m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m    746\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[1;32m    747\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    748\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import neuralprocesses.torch as nps\n",
    "import torch.nn.functional as F\n",
    "\n",
    "sampler = Sampler(grid_size = 10, agent_view_size = 3, traj_length = 10)\n",
    "\n",
    "if training:\n",
    "  cnp = nps.construct_agnp(dim_x = 2, dim_y = 5, likelihood = \"het\", dim_embedding = 64).to(\"cpu\")\n",
    "  #cnp = nps.construct_convgnp(dim_x = 2, dim_y = 5).to(\"cpu\")\n",
    "  opt = torch.optim.Adam(cnp.parameters(), lr = 1e-4)\n",
    "  total_loss = 0\n",
    "  for i in range(10000):\n",
    "    xc, yc, xt, yt = sampler.generate_batch([0.4,0.5])\n",
    "\n",
    "    xc = xc.permute(0,2,1).to(\"cpu\")\n",
    "    yc = yc.permute(0,2,1).to(\"cpu\")\n",
    "    xt = xt.permute(0,2,1).to(\"cpu\")\n",
    "    yt = yt.permute(0,2,1).to(\"cpu\")\n",
    "    \n",
    "    dist = cnp(xc, yc, xt)\n",
    "    #mean = dist.mean.permute(0,2,1)\n",
    "    #yt_indices = torch.argmax(yt, dim = -1)\n",
    "    #loss = F.cross_entropy(dist.mean, yt_indices)\n",
    "    \n",
    "    loss = -torch.mean(nps.loglik(cnp, xc, yc, xt, yt))\n",
    "    opt.zero_grad(set_to_none=True)\n",
    "    loss.backward()\n",
    "    opt.step()\n",
    "    \n",
    "    total_loss += loss.item()\n",
    "    \n",
    "    if i % 100 == 0:\n",
    "      print(f\"mean loss {total_loss/(i+1)}\")\n",
    "      total_loss = 0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "if training:\n",
    "  PATH = os.path.abspath(os.getcwd())\n",
    "  torch.save(cnp.state_dict(), PATH + \"/models/anp.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Model(\n",
       "    Chain(\n",
       "        Chain(\n",
       "            SqueezeParallel(),\n",
       "            AssertNoParallel(),\n",
       "        ),\n",
       "        Copy(),\n",
       "        Parallel(\n",
       "            Chain(\n",
       "                RepeatForAggregateInputs(\n",
       "                  (coder): InputsCoder()\n",
       "                ),\n",
       "                DeterministicLikelihood(),\n",
       "            ),\n",
       "            Parallel(\n",
       "                Chain(\n",
       "                    RepeatForAggregateInputs(\n",
       "                      (coder): Attention(\n",
       "                        (encoder_x): MLP(\n",
       "                          (net): Sequential(\n",
       "                            (0): Linear(in_features=2, out_features=256, bias=True)\n",
       "                            (1): ReLU()\n",
       "                            (2): Linear(in_features=256, out_features=256, bias=True)\n",
       "                            (3): ReLU()\n",
       "                            (4): Linear(in_features=256, out_features=256, bias=True)\n",
       "                            (5): ReLU()\n",
       "                            (6): Linear(in_features=256, out_features=256, bias=True)\n",
       "                          )\n",
       "                        )\n",
       "                        (encoder_xy): MLP(\n",
       "                          (net): Sequential(\n",
       "                            (0): Linear(in_features=7, out_features=256, bias=True)\n",
       "                            (1): ReLU()\n",
       "                            (2): Linear(in_features=256, out_features=256, bias=True)\n",
       "                            (3): ReLU()\n",
       "                            (4): Linear(in_features=256, out_features=256, bias=True)\n",
       "                            (5): ReLU()\n",
       "                            (6): Linear(in_features=256, out_features=256, bias=True)\n",
       "                          )\n",
       "                        )\n",
       "                        (mixer): MLP(\n",
       "                          (net): Linear(in_features=256, out_features=256, bias=True)\n",
       "                        )\n",
       "                        (mlp1): MLP(\n",
       "                          (net): Linear(in_features=256, out_features=256, bias=True)\n",
       "                        )\n",
       "                        (ln1): Sequential(\n",
       "                          (0): _LambdaModule()\n",
       "                          (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "                          (2): _LambdaModule()\n",
       "                        )\n",
       "                        (mlp2): MLP(\n",
       "                          (net): Sequential(\n",
       "                            (0): Linear(in_features=256, out_features=256, bias=True)\n",
       "                            (1): ReLU()\n",
       "                            (2): Linear(in_features=256, out_features=256, bias=True)\n",
       "                          )\n",
       "                        )\n",
       "                        (ln2): Sequential(\n",
       "                          (0): _LambdaModule()\n",
       "                          (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "                          (2): _LambdaModule()\n",
       "                        )\n",
       "                      )\n",
       "                    ),\n",
       "                    DeterministicLikelihood(),\n",
       "                ),\n",
       "            ),\n",
       "        ),\n",
       "    ),\n",
       "    Chain(\n",
       "        Concatenate(),\n",
       "        RepeatForAggregateInputs(\n",
       "          (coder): Chain(\n",
       "              MLP(\n",
       "                (net): Sequential(\n",
       "                  (0): Linear(in_features=258, out_features=512, bias=True)\n",
       "                  (1): ReLU()\n",
       "                  (2): Linear(in_features=512, out_features=512, bias=True)\n",
       "                  (3): ReLU()\n",
       "                  (4): Linear(in_features=512, out_features=512, bias=True)\n",
       "                  (5): ReLU()\n",
       "                  (6): Linear(in_features=512, out_features=512, bias=True)\n",
       "                  (7): ReLU()\n",
       "                  (8): Linear(in_features=512, out_features=512, bias=True)\n",
       "                  (9): ReLU()\n",
       "                  (10): Linear(in_features=512, out_features=512, bias=True)\n",
       "                  (11): ReLU()\n",
       "                  (12): Linear(in_features=512, out_features=10, bias=True)\n",
       "                )\n",
       "              ),\n",
       "              SelectFromChannels(),\n",
       "          )\n",
       "        ),\n",
       "        HeterogeneousGaussianLikelihood(epsilon=1e-06),\n",
       "        _LambdaModule(),\n",
       "    )\n",
       ")"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "if not training:\n",
    "  cnp = nps.construct_agnp(dim_x = 2, dim_y = 4, likelihood = \"het\").to(device)\n",
    "  cnp.load_state_dict(torch.load('models/anp.pth'))\n",
    "\n",
    "cnp.eval().to(\"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make predictions on new trajectories, and compare"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnp.eval().to(\"cpu\")\n",
    "xc, yc, xt, yt = sampler.generate_batch()\n",
    "\n",
    "xc = xc.permute(0,2,1).to(\"cpu\")\n",
    "yc = yc.permute(0,2,1).to(\"cpu\")\n",
    "xt = xt.permute(0,2,1).to(\"cpu\")\n",
    "yt = yt.permute(0,2,1).to(\"cpu\")\n",
    "\n",
    "mean, var, noiseless_samples, noisy_samples = nps.predict(cnp, xc, yc, xt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([25, 2, 1])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xt.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[[0.1961],\n",
       "          [0.2020],\n",
       "          [0.1333],\n",
       "          [0.2546],\n",
       "          [0.2635]],\n",
       " \n",
       "         [[0.1961],\n",
       "          [0.2020],\n",
       "          [0.1333],\n",
       "          [0.2546],\n",
       "          [0.2635]],\n",
       " \n",
       "         [[0.1961],\n",
       "          [0.2020],\n",
       "          [0.1333],\n",
       "          [0.2546],\n",
       "          [0.2635]],\n",
       " \n",
       "         [[0.1961],\n",
       "          [0.2020],\n",
       "          [0.1333],\n",
       "          [0.2546],\n",
       "          [0.2635]],\n",
       " \n",
       "         [[0.1961],\n",
       "          [0.2020],\n",
       "          [0.1333],\n",
       "          [0.2546],\n",
       "          [0.2635]],\n",
       " \n",
       "         [[0.1836],\n",
       "          [0.1905],\n",
       "          [0.1300],\n",
       "          [0.2316],\n",
       "          [0.2411]],\n",
       " \n",
       "         [[0.1836],\n",
       "          [0.1905],\n",
       "          [0.1300],\n",
       "          [0.2316],\n",
       "          [0.2411]],\n",
       " \n",
       "         [[0.1836],\n",
       "          [0.1905],\n",
       "          [0.1300],\n",
       "          [0.2316],\n",
       "          [0.2411]],\n",
       " \n",
       "         [[0.1836],\n",
       "          [0.1905],\n",
       "          [0.1300],\n",
       "          [0.2316],\n",
       "          [0.2411]],\n",
       " \n",
       "         [[0.1836],\n",
       "          [0.1905],\n",
       "          [0.1300],\n",
       "          [0.2316],\n",
       "          [0.2411]],\n",
       " \n",
       "         [[0.1806],\n",
       "          [0.1775],\n",
       "          [0.1244],\n",
       "          [0.2202],\n",
       "          [0.2272]],\n",
       " \n",
       "         [[0.1806],\n",
       "          [0.1775],\n",
       "          [0.1244],\n",
       "          [0.2202],\n",
       "          [0.2272]],\n",
       " \n",
       "         [[0.1806],\n",
       "          [0.1775],\n",
       "          [0.1244],\n",
       "          [0.2202],\n",
       "          [0.2272]],\n",
       " \n",
       "         [[0.1806],\n",
       "          [0.1775],\n",
       "          [0.1244],\n",
       "          [0.2202],\n",
       "          [0.2272]],\n",
       " \n",
       "         [[0.1806],\n",
       "          [0.1775],\n",
       "          [0.1244],\n",
       "          [0.2202],\n",
       "          [0.2272]],\n",
       " \n",
       "         [[0.2031],\n",
       "          [0.2121],\n",
       "          [0.1370],\n",
       "          [0.2690],\n",
       "          [0.2787]],\n",
       " \n",
       "         [[0.2031],\n",
       "          [0.2121],\n",
       "          [0.1370],\n",
       "          [0.2690],\n",
       "          [0.2787]],\n",
       " \n",
       "         [[0.2031],\n",
       "          [0.2121],\n",
       "          [0.1370],\n",
       "          [0.2690],\n",
       "          [0.2787]],\n",
       " \n",
       "         [[0.2031],\n",
       "          [0.2121],\n",
       "          [0.1370],\n",
       "          [0.2690],\n",
       "          [0.2787]],\n",
       " \n",
       "         [[0.2031],\n",
       "          [0.2121],\n",
       "          [0.1370],\n",
       "          [0.2690],\n",
       "          [0.2787]],\n",
       " \n",
       "         [[0.1869],\n",
       "          [0.1804],\n",
       "          [0.1248],\n",
       "          [0.2294],\n",
       "          [0.2359]],\n",
       " \n",
       "         [[0.1869],\n",
       "          [0.1804],\n",
       "          [0.1248],\n",
       "          [0.2294],\n",
       "          [0.2359]],\n",
       " \n",
       "         [[0.1869],\n",
       "          [0.1804],\n",
       "          [0.1248],\n",
       "          [0.2294],\n",
       "          [0.2359]],\n",
       " \n",
       "         [[0.1869],\n",
       "          [0.1804],\n",
       "          [0.1248],\n",
       "          [0.2294],\n",
       "          [0.2359]],\n",
       " \n",
       "         [[0.1869],\n",
       "          [0.1804],\n",
       "          [0.1248],\n",
       "          [0.2294],\n",
       "          [0.2359]],\n",
       " \n",
       "         [[0.1861],\n",
       "          [0.1961],\n",
       "          [0.1326],\n",
       "          [0.2383],\n",
       "          [0.2485]],\n",
       " \n",
       "         [[0.1861],\n",
       "          [0.1961],\n",
       "          [0.1326],\n",
       "          [0.2383],\n",
       "          [0.2485]],\n",
       " \n",
       "         [[0.1861],\n",
       "          [0.1961],\n",
       "          [0.1326],\n",
       "          [0.2383],\n",
       "          [0.2485]],\n",
       " \n",
       "         [[0.1861],\n",
       "          [0.1961],\n",
       "          [0.1326],\n",
       "          [0.2383],\n",
       "          [0.2485]],\n",
       " \n",
       "         [[0.1861],\n",
       "          [0.1961],\n",
       "          [0.1326],\n",
       "          [0.2383],\n",
       "          [0.2485]],\n",
       " \n",
       "         [[0.1798],\n",
       "          [0.1707],\n",
       "          [0.1217],\n",
       "          [0.2146],\n",
       "          [0.2210]],\n",
       " \n",
       "         [[0.1798],\n",
       "          [0.1707],\n",
       "          [0.1217],\n",
       "          [0.2146],\n",
       "          [0.2210]],\n",
       " \n",
       "         [[0.1798],\n",
       "          [0.1707],\n",
       "          [0.1217],\n",
       "          [0.2146],\n",
       "          [0.2210]],\n",
       " \n",
       "         [[0.1798],\n",
       "          [0.1707],\n",
       "          [0.1217],\n",
       "          [0.2146],\n",
       "          [0.2210]],\n",
       " \n",
       "         [[0.1798],\n",
       "          [0.1707],\n",
       "          [0.1217],\n",
       "          [0.2146],\n",
       "          [0.2210]]], dtype=torch.float64, grad_fn=<SqueezeBackward1>),\n",
       " tensor([[[0.],\n",
       "          [1.],\n",
       "          [0.],\n",
       "          [0.],\n",
       "          [0.]],\n",
       " \n",
       "         [[0.],\n",
       "          [1.],\n",
       "          [0.],\n",
       "          [0.],\n",
       "          [0.]],\n",
       " \n",
       "         [[0.],\n",
       "          [1.],\n",
       "          [0.],\n",
       "          [0.],\n",
       "          [0.]],\n",
       " \n",
       "         [[0.],\n",
       "          [1.],\n",
       "          [0.],\n",
       "          [0.],\n",
       "          [0.]],\n",
       " \n",
       "         [[0.],\n",
       "          [1.],\n",
       "          [0.],\n",
       "          [0.],\n",
       "          [0.]],\n",
       " \n",
       "         [[0.],\n",
       "          [0.],\n",
       "          [0.],\n",
       "          [0.],\n",
       "          [1.]],\n",
       " \n",
       "         [[0.],\n",
       "          [0.],\n",
       "          [0.],\n",
       "          [0.],\n",
       "          [1.]],\n",
       " \n",
       "         [[0.],\n",
       "          [0.],\n",
       "          [0.],\n",
       "          [0.],\n",
       "          [1.]],\n",
       " \n",
       "         [[0.],\n",
       "          [0.],\n",
       "          [0.],\n",
       "          [0.],\n",
       "          [1.]],\n",
       " \n",
       "         [[0.],\n",
       "          [0.],\n",
       "          [0.],\n",
       "          [0.],\n",
       "          [1.]],\n",
       " \n",
       "         [[1.],\n",
       "          [0.],\n",
       "          [0.],\n",
       "          [0.],\n",
       "          [0.]],\n",
       " \n",
       "         [[1.],\n",
       "          [0.],\n",
       "          [0.],\n",
       "          [0.],\n",
       "          [0.]],\n",
       " \n",
       "         [[1.],\n",
       "          [0.],\n",
       "          [0.],\n",
       "          [0.],\n",
       "          [0.]],\n",
       " \n",
       "         [[1.],\n",
       "          [0.],\n",
       "          [0.],\n",
       "          [0.],\n",
       "          [0.]],\n",
       " \n",
       "         [[1.],\n",
       "          [0.],\n",
       "          [0.],\n",
       "          [0.],\n",
       "          [0.]],\n",
       " \n",
       "         [[0.],\n",
       "          [0.],\n",
       "          [0.],\n",
       "          [0.],\n",
       "          [1.]],\n",
       " \n",
       "         [[0.],\n",
       "          [0.],\n",
       "          [0.],\n",
       "          [0.],\n",
       "          [1.]],\n",
       " \n",
       "         [[0.],\n",
       "          [0.],\n",
       "          [0.],\n",
       "          [0.],\n",
       "          [1.]],\n",
       " \n",
       "         [[0.],\n",
       "          [0.],\n",
       "          [0.],\n",
       "          [0.],\n",
       "          [1.]],\n",
       " \n",
       "         [[0.],\n",
       "          [0.],\n",
       "          [0.],\n",
       "          [0.],\n",
       "          [1.]],\n",
       " \n",
       "         [[1.],\n",
       "          [0.],\n",
       "          [0.],\n",
       "          [0.],\n",
       "          [0.]],\n",
       " \n",
       "         [[1.],\n",
       "          [0.],\n",
       "          [0.],\n",
       "          [0.],\n",
       "          [0.]],\n",
       " \n",
       "         [[1.],\n",
       "          [0.],\n",
       "          [0.],\n",
       "          [0.],\n",
       "          [0.]],\n",
       " \n",
       "         [[1.],\n",
       "          [0.],\n",
       "          [0.],\n",
       "          [0.],\n",
       "          [0.]],\n",
       " \n",
       "         [[1.],\n",
       "          [0.],\n",
       "          [0.],\n",
       "          [0.],\n",
       "          [0.]],\n",
       " \n",
       "         [[0.],\n",
       "          [0.],\n",
       "          [0.],\n",
       "          [0.],\n",
       "          [1.]],\n",
       " \n",
       "         [[0.],\n",
       "          [0.],\n",
       "          [0.],\n",
       "          [0.],\n",
       "          [1.]],\n",
       " \n",
       "         [[0.],\n",
       "          [0.],\n",
       "          [0.],\n",
       "          [0.],\n",
       "          [1.]],\n",
       " \n",
       "         [[0.],\n",
       "          [0.],\n",
       "          [0.],\n",
       "          [0.],\n",
       "          [1.]],\n",
       " \n",
       "         [[0.],\n",
       "          [0.],\n",
       "          [0.],\n",
       "          [0.],\n",
       "          [1.]],\n",
       " \n",
       "         [[0.],\n",
       "          [0.],\n",
       "          [1.],\n",
       "          [0.],\n",
       "          [0.]],\n",
       " \n",
       "         [[0.],\n",
       "          [0.],\n",
       "          [1.],\n",
       "          [0.],\n",
       "          [0.]],\n",
       " \n",
       "         [[0.],\n",
       "          [0.],\n",
       "          [1.],\n",
       "          [0.],\n",
       "          [0.]],\n",
       " \n",
       "         [[0.],\n",
       "          [0.],\n",
       "          [1.],\n",
       "          [0.],\n",
       "          [0.]],\n",
       " \n",
       "         [[0.],\n",
       "          [0.],\n",
       "          [1.],\n",
       "          [0.],\n",
       "          [0.]]]))"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mean, yt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [1.]],\n",
       "\n",
       "        [[0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [1.]],\n",
       "\n",
       "        [[0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [1.]],\n",
       "\n",
       "        [[0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [1.]],\n",
       "\n",
       "        [[0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [1.]],\n",
       "\n",
       "        [[0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [1.]],\n",
       "\n",
       "        [[0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [1.]],\n",
       "\n",
       "        [[0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [1.]],\n",
       "\n",
       "        [[0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [1.]],\n",
       "\n",
       "        [[0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [1.]],\n",
       "\n",
       "        [[0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [1.]],\n",
       "\n",
       "        [[0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [1.]],\n",
       "\n",
       "        [[0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [1.]],\n",
       "\n",
       "        [[0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [1.]],\n",
       "\n",
       "        [[0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [1.]],\n",
       "\n",
       "        [[0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [1.]],\n",
       "\n",
       "        [[0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [1.]],\n",
       "\n",
       "        [[0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [1.]],\n",
       "\n",
       "        [[0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [1.]],\n",
       "\n",
       "        [[0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [1.]],\n",
       "\n",
       "        [[0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [1.]],\n",
       "\n",
       "        [[0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [1.]],\n",
       "\n",
       "        [[0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [1.]],\n",
       "\n",
       "        [[0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [1.]],\n",
       "\n",
       "        [[0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [1.]],\n",
       "\n",
       "        [[0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [1.]],\n",
       "\n",
       "        [[0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [1.]],\n",
       "\n",
       "        [[0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [1.]],\n",
       "\n",
       "        [[0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [1.]],\n",
       "\n",
       "        [[0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [1.]],\n",
       "\n",
       "        [[0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [1.]],\n",
       "\n",
       "        [[0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [1.]],\n",
       "\n",
       "        [[0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [1.]],\n",
       "\n",
       "        [[0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [1.]],\n",
       "\n",
       "        [[0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [1.]],\n",
       "\n",
       "        [[0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [1.]],\n",
       "\n",
       "        [[0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [1.]],\n",
       "\n",
       "        [[0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [1.]],\n",
       "\n",
       "        [[0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [1.]],\n",
       "\n",
       "        [[0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [1.]],\n",
       "\n",
       "        [[0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [1.]],\n",
       "\n",
       "        [[0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [1.]],\n",
       "\n",
       "        [[0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [1.]],\n",
       "\n",
       "        [[0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [1.]],\n",
       "\n",
       "        [[0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [1.]],\n",
       "\n",
       "        [[0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [1.]],\n",
       "\n",
       "        [[0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [1.]],\n",
       "\n",
       "        [[0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [1.]],\n",
       "\n",
       "        [[0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [1.]],\n",
       "\n",
       "        [[0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [1.]],\n",
       "\n",
       "        [[0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [1.]],\n",
       "\n",
       "        [[0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [1.]],\n",
       "\n",
       "        [[0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [1.]],\n",
       "\n",
       "        [[0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [1.]],\n",
       "\n",
       "        [[0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [1.]],\n",
       "\n",
       "        [[0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [1.]],\n",
       "\n",
       "        [[0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [1.]],\n",
       "\n",
       "        [[0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [1.]],\n",
       "\n",
       "        [[0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [1.]],\n",
       "\n",
       "        [[0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [1.]]])"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "yt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The following is a copy from the relational_neural_process githu\n",
    "\n",
    "Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import numpy.random as npr\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import collections\n",
    "import matplotlib.pyplot as plt\n",
    "import datetime\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class CNPDeterministicEncoder(nn.Module):\n",
    "    def __init__(self, sizes):\n",
    "        super(CNPDeterministicEncoder, self).__init__()\n",
    "        self.linears = nn.ModuleList()\n",
    "        for i in range(len(sizes) - 1):\n",
    "            self.linears.append(nn.Linear(sizes[i], sizes[i + 1]))\n",
    "\n",
    "    def forward(self, context_x, context_y):\n",
    "        \"\"\"\n",
    "        Encode training set as one vector representation\n",
    "\n",
    "        Args:\n",
    "            context_x: batch_size x set_size x feature_dim_x\n",
    "            context_y: batch_size x set_size x feature_dim_y\n",
    "\n",
    "        Returns: representation: batch_size x representation_size:\n",
    "        \"\"\"\n",
    "\n",
    "        encoder_input = torch.cat((context_x, context_y), dim = -1)\n",
    "        batch_size, set_size, filter_size = encoder_input.shape\n",
    "        x = encoder_input.view(batch_size * set_size, -1)\n",
    "        for i, linear in enumerate(self.linears[:-1]):\n",
    "            x = torch.relu(linear(x))\n",
    "        x = self.linears[-1](x)\n",
    "        x = x.view(batch_size, set_size, -1)\n",
    "        representation = x.sum(dim=1)\n",
    "        return representation\n",
    "            \n",
    "class CNPDeterministicDecoder(nn.Module):\n",
    "    def __init__(self, sizes):\n",
    "        super(CNPDeterministicDecoder, self).__init__()\n",
    "        self.linears = nn.ModuleList()\n",
    "        for i in range(len(sizes) - 1):\n",
    "            self.linears.append(nn.Linear(sizes[i], sizes[i + 1]))\n",
    "\n",
    "    def forward(self, representation, target_x):\n",
    "        \"\"\"\n",
    "        Take representation representation of current training set, and a target input x,\n",
    "        return the predictive distribution at x (Gaussian with mean mu and scale sigma)\n",
    "\n",
    "        Args:\n",
    "            representation: batch_size x representation_size\n",
    "            target_x: batch_size x set_size x d\n",
    "        \"\"\"\n",
    "        batch_size, set_size, d = target_x.shape\n",
    "        \n",
    "        if representation is None:        \n",
    "            input = target_x            \n",
    "        else:\n",
    "            representation = representation.unsqueeze(1).repeat([1, set_size, 1])\n",
    "            input = torch.cat((representation, target_x), dim=-1)\n",
    "        \n",
    "        #All rows\n",
    "        x = input.view(batch_size * set_size, -1)\n",
    "        for linear in self.linears[:-1]:\n",
    "            x = torch.relu(linear(x))\n",
    "        logits = self.linears[-1](x)\n",
    "        logits = logits.view(batch_size, set_size, -1)\n",
    "        probs = F.softmax(logits, dim = -1)\n",
    "\n",
    "        dist = torch.distributions.categorical.Categorical(probs = probs)\n",
    "        return dist, probs, logits\n",
    "    \n",
    "        '''\n",
    "        mu, log_sigma = torch.split(out, 1, dim = -1)\n",
    "        sigma = 0.01 + 0.99 * torch.nn.functional.softplus(log_sigma)\n",
    "        dist = torch.distributions.normal.Normal(loc=mu, scale=sigma)\n",
    "        '''\n",
    "\n",
    "class CNPDeterministicModel(nn.Module):\n",
    "    def __init__(self, encoder_size, decoder_size):\n",
    "        super(CNPDeterministicModel, self).__init__()\n",
    "        self._encoder = CNPDeterministicEncoder(encoder_size)\n",
    "        self._decoder = CNPDeterministicDecoder(decoder_size)\n",
    "\n",
    "\n",
    "    def forward(self, query, target_y = None):\n",
    "        (context_x, context_y), target_x = query\n",
    "        representation = self._encoder(context_x, context_y)\n",
    "        dist, probs, logits = self._decoder(representation, target_x)\n",
    "\n",
    "        log_p = None\n",
    "        if target_y is not None:\n",
    "            #Reverse one hot encoding on target_y\n",
    "            target_y = torch.argmax(target_y, dim = -1)\n",
    "            log_p = dist.log_prob(target_y)\n",
    "\n",
    "        return log_p, probs, logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CNPDeterministicModel(\n",
       "  (_encoder): CNPDeterministicEncoder(\n",
       "    (linears): ModuleList(\n",
       "      (0): Linear(in_features=6, out_features=128, bias=True)\n",
       "      (1-2): 2 x Linear(in_features=128, out_features=128, bias=True)\n",
       "      (3): Linear(in_features=128, out_features=258, bias=True)\n",
       "    )\n",
       "  )\n",
       "  (_decoder): CNPDeterministicDecoder(\n",
       "    (linears): ModuleList(\n",
       "      (0): Linear(in_features=260, out_features=128, bias=True)\n",
       "      (1-2): 2 x Linear(in_features=128, out_features=128, bias=True)\n",
       "      (3): Linear(in_features=128, out_features=4, bias=True)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import neuralprocesses.torch as nps\n",
    "\n",
    "d_x, d_in, representation_size, d_out, hidden_size = 2, 6, 258, 4, 128\n",
    "encoder_sizes = [d_in, hidden_size, hidden_size, hidden_size, representation_size]\n",
    "decoder_sizes = [representation_size + d_x, hidden_size, hidden_size, hidden_size, d_out]\n",
    "\n",
    "model = CNPDeterministicModel(encoder_size=encoder_sizes, decoder_size=decoder_sizes)\n",
    "model.to(device)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pml2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
