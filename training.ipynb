{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "\n",
    "USE_CUDA = torch.cuda.is_available()\n",
    "device = torch.device(\"cuda\") if USE_CUDA else torch.device(\"cpu\")\n",
    "training = True\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Constructing datasets\n",
    "\n",
    "The Sampler-class is used to extract the simulated user trajectories. Below is an example data processing pipeline using the class.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_context_and_target(trajectories):\n",
    "  xc, yc, xt, yt = [], [], [], []\n",
    "  traj_length = len(trajectories[0])\n",
    "  half_point = traj_length // 2\n",
    "  \n",
    "  for i, traj in enumerate(trajectories):\n",
    "\n",
    "    #Context is each context but the current index\n",
    "    context = [trajectories[j] for j in range(len(trajectories)) if j != i]\n",
    "    \n",
    "    #All except last\n",
    "    #context += [traj[:-1]]\n",
    "    \n",
    "    #Half\n",
    "    context += [traj[:half_point]]\n",
    "    context_s, context_a = zip(*[point for c in context for point in c])\n",
    "    \n",
    "    #Only predict the last step\n",
    "    #target_s, target_a = zip(*[traj[-1]])\n",
    "    \n",
    "    #Target last half\n",
    "    target_s, target_a = zip(*traj[half_point:])\n",
    "    \n",
    "    xc.append(torch.tensor(context_s, dtype=torch.float32))\n",
    "    yc.append(torch.tensor(context_a, dtype=torch.float32))\n",
    "    xt.append(torch.tensor(target_s, dtype=torch.float32))\n",
    "    yt.append(torch.tensor(target_a, dtype=torch.float32))\n",
    "    \n",
    "  # Stack tensors\n",
    "  xc = torch.stack(xc).to(device)\n",
    "  yc = torch.stack(yc).to(device)\n",
    "  xt = torch.stack(xt).to(device)\n",
    "  yt = torch.stack(yt).to(device)\n",
    "\n",
    "  return xc, yc, xt, yt\n",
    "  \n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 2]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kakka = [1,2,3]\n",
    "\n",
    "kakka[:-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.utils.sampler import Sampler\n",
    "import numpy as np\n",
    "\n",
    "def get_batch(sampler, device = device):\n",
    "\n",
    "  import numpy as np\n",
    "  user_params = sampler.generate_user_parameters()\n",
    "\n",
    "  n_trajectories = np.random.randint(low = 5, high = 10)\n",
    "\n",
    "  trajectories = sampler.generate_user_trajectories(n_trajectories, user_params)\n",
    "\n",
    "  xc, yc, xt, yt = build_context_and_target(trajectories)\n",
    "  \n",
    "  xc = xc.permute(0,2,1).to(device)\n",
    "  yc = yc.permute(0,2,1).to(device)\n",
    "  xt = xt.permute(0,2,1).to(device)\n",
    "  yt = yt.permute(0,2,1).to(device)\n",
    "  \n",
    "  return xc, yc, xt, yt, user_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2, 2)\n",
      "Shape of xc: torch.Size([6, 2, 55]), yc: torch.Size([6, 5, 55]), xt: torch.Size([6, 2, 5]), yt: torch.Size([6, 5, 5])\n",
      "Device of xc: cuda:0, yc: cuda:0, xt: cuda:0, yt: cuda:0\n",
      "First sequence context:\n",
      "tensor([[ 7.,  6.,  5.,  4.,  3.,  2.,  2.,  2.,  2.,  2.,  9.,  9.,  8.,  7.,\n",
      "          6.,  5.,  4.,  3.,  3.,  2.,  5.,  4.,  3.,  3.,  2.,  2.,  2.,  2.,\n",
      "          2.,  2.,  7.,  6.,  5.,  4.,  3.,  3.,  2.,  2.,  2.,  2.,  5.,  4.,\n",
      "          3.,  2.,  2.,  2.,  2.,  2.,  2.,  2., 10.,  9.,  8.,  7.,  6.],\n",
      "        [ 9.,  9.,  9.,  9.,  9.,  9., 10.,  9.,  8.,  7.,  2.,  1.,  1.,  1.,\n",
      "          1.,  1.,  1.,  1.,  2.,  2.,  6.,  6.,  6.,  7.,  7.,  6.,  5.,  4.,\n",
      "          3.,  2.,  4.,  4.,  4.,  4.,  4.,  5.,  5.,  4.,  3.,  2.,  3.,  3.,\n",
      "          3.,  3.,  2.,  2.,  2.,  2.,  2.,  2.,  8.,  8.,  8.,  8.,  8.]],\n",
      "       device='cuda:0')\n",
      "Goal position: (2, 2)\n"
     ]
    }
   ],
   "source": [
    "from src.utils.sampler import Sampler\n",
    "\n",
    "GRID_SIZE = 10\n",
    "AGENT_VIEW_SIZE = 3\n",
    "TRAJ_LENGTH = 10\n",
    "\n",
    "sampler = Sampler(grid_size = GRID_SIZE, agent_view_size = AGENT_VIEW_SIZE, traj_length = TRAJ_LENGTH)\n",
    "xc, yc, xt, yt, user_params = get_batch(sampler)\n",
    "\n",
    "print(user_params[\"goal_position\"])\n",
    "\n",
    "print(f\"Shape of xc: {xc.shape}, yc: {yc.shape}, xt: {xt.shape}, yt: {yt.shape}\")\n",
    "print(f\"Device of xc: {xc.device}, yc: {yc.device}, xt: {xt.device}, yt: {yt.device}\")\n",
    "\n",
    "print(f\"First sequence context:\\n{xc[0]}\")\n",
    "print(f\"Goal position: {user_params['goal_position']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing\n",
    "\n",
    "Change: Use cross entropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(22.4112, device='cuda:0', dtype=torch.float64, grad_fn=<NegBackward0>) tensor(1.6226, device='cuda:0', grad_fn=<MeanBackward0>)\n"
     ]
    }
   ],
   "source": [
    "import neuralprocesses.torch as nps\n",
    "from neuralprocesses.torch import MultiOutputNormal\n",
    "import torch.nn.functional as F\n",
    "\n",
    "device = \"cuda\"\n",
    "\n",
    "xc, yc, xt, yt, _ = get_batch(sampler)\n",
    "\n",
    "agnp = nps.construct_agnp(dim_x = 2, dim_y = 5, likelihood = \"het\").to(device)\n",
    "\n",
    "dist = agnp(xc, yc, xt)\n",
    "\n",
    "logits = dist.mean  \n",
    "log_probs = F.log_softmax(logits, dim=-2)  # Apply log softmax to get log-probabilities\n",
    "\n",
    "nll = -(log_probs * yt).sum(dim=-2)  # Sum over the class dimension\n",
    "loss = nll.mean()\n",
    "\n",
    "\n",
    "elbo = -nps.elbo(agnp, xc, yc, xt, yt).mean()\n",
    "\n",
    "\n",
    "print(elbo, loss)\n",
    "\n",
    "#samples.shape, yt.shape\n",
    "#Testing \n",
    "#print((predictions == yt.argmax(-2)).float().mean())\n",
    "#print((predictions*yt.argmax(-2)).float().mean(),(predictions == yt.argmax(-2)).float().mean())\n",
    "#print(predictions.shape, yt.argmax(-2).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([6, 5]) torch.Size([6, 5])\n",
      "Predictions: tensor([[3, 3, 3, 3, 3],\n",
      "        [3, 3, 3, 3, 3],\n",
      "        [3, 3, 3, 3, 3],\n",
      "        [3, 3, 3, 3, 3],\n",
      "        [1, 1, 1, 3, 3],\n",
      "        [1, 3, 3, 3, 3]], device='cuda:0')\n",
      "True actions: tensor([[4, 4, 4, 4, 4],\n",
      "        [2, 2, 4, 4, 4],\n",
      "        [4, 4, 4, 4, 4],\n",
      "        [0, 2, 0, 3, 3],\n",
      "        [0, 0, 2, 0, 0],\n",
      "        [2, 2, 2, 2, 4]], device='cuda:0')\n",
      "Manhattan distance: tensor([15., 24., 15., 16., 18., 21.], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "from minigrid.core.constants import DIR_TO_VEC\n",
    "#The last action corresponding to stationary\n",
    "DIR_TO_VEC =  DIR_TO_VEC + [(0, 0)]\n",
    "\n",
    "def construct_trajectory(actions, start_pos = None):\n",
    "    \"\"\"\n",
    "    Constructs a trajectory from a sequence of actions\n",
    "    \"\"\"\n",
    "    \n",
    "    mapping = torch.tensor(DIR_TO_VEC, dtype = torch.float32, device = actions.device)\n",
    "    \n",
    "    batch_size, seq_length = actions.shape\n",
    "    \n",
    "    trajectory = torch.zeros(batch_size, 2, seq_length, device = actions.device)\n",
    "    \n",
    "    if start_pos is not None:\n",
    "        prev = start_pos\n",
    "    else:\n",
    "        prev = torch.zeros(batch_size, 2, device = actions.device)\n",
    "        \n",
    "    for i in range(seq_length):\n",
    "        action_indices = actions[:, i]\n",
    "        change = mapping[action_indices]\n",
    "        trajectory[:, :, i] = prev + change\n",
    "        prev = trajectory[:, :, i]\n",
    "    \n",
    "    return trajectory\n",
    "    \n",
    "def construct_and_calc_l1_dist(yt, predictions, xc=None):\n",
    "    '''\n",
    "    Calculates the Manhattan distance between the predicted and true trajectories.\n",
    "    '''\n",
    "    # Construct trajectories\n",
    "    start_pos = xc[:, :, -1] if xc is not None else None\n",
    "    true_trajectory = construct_trajectory(yt, start_pos)\n",
    "    pred_trajectory = construct_trajectory(predictions, start_pos)\n",
    "    \n",
    "    # Calculate Manhattan distance at each timestep\n",
    "    distance = torch.abs(pred_trajectory - true_trajectory).sum(dim=1)\n",
    "    \n",
    "    # Sum distances across all timesteps\n",
    "    total_distance = distance.sum(dim=-1)\n",
    "    \n",
    "    return total_distance\n",
    "\n",
    "# Test\n",
    "xc, yc, xt, yt, _ = get_batch(sampler)\n",
    "dist = agnp(xc, yc, xt).mean\n",
    "preds = F.softmax(dist, dim=-2)\n",
    "true_actions = yt.argmax(-2)\n",
    "predictions = preds.argmax(-2)\n",
    "print(true_actions.shape, predictions.shape)\n",
    "distance = construct_and_calc_l1_dist(true_actions, predictions)\n",
    "\n",
    "\n",
    "print(f\"Predictions: {predictions}\")\n",
    "print(f\"True actions: {true_actions}\")\n",
    "print(f\"Manhattan distance: {distance}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0., 0., 0., 0., 0., 0.], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "from neuralprocesses.coding import code,code_track, recode_stochastic\n",
    "from neuralprocesses.model.util import compress_contexts\n",
    "from neuralprocesses import _dispatch\n",
    "from neuralprocesses.parallel import Parallel\n",
    "from neuralprocesses.dist import AbstractDistribution\n",
    "\n",
    "@_dispatch\n",
    "def _kl(q: AbstractDistribution, p: AbstractDistribution):\n",
    "    return q.kl(p)\n",
    "\n",
    "\n",
    "@_dispatch\n",
    "def _kl(q: Parallel, p: Parallel):\n",
    "    return sum([_kl(qi, pi) for qi, pi in zip(q, p)])\n",
    "\n",
    "\n",
    "def calc_kl_divergence(model, xc, yc, xt, yt, dtype_lik = None):\n",
    "  \n",
    "    if not dtype_lik:\n",
    "      dtype_lik = torch.float32\n",
    "    \n",
    "    all_x = torch.cat([xc, xt], dim = -1)\n",
    "    all_y = torch.cat([yc, yt], dim = -1)\n",
    "      \n",
    "    xz, pz, h = code_track(model.encoder, xc, yc, xt, root=True)\n",
    "    \n",
    "    qz  = recode_stochastic(model.encoder, pz, all_x, all_y, h, root=True, dtype_lik = dtype_lik)\n",
    "    \n",
    "    kl = _kl(qz, pz)\n",
    "    \n",
    "    return kl\n",
    "\n",
    "kl = calc_kl_divergence(agnp, xc, yc, xt, yt)\n",
    "print(kl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 0, Loss: 1.6182, Accuracy: 0.0857 mean distance difference: 20.5714, Final distance difference: 13.0000\n",
      "Iteration 100, Loss: 1.5501, Accuracy: 0.5000 mean distance difference: 8.6667, Final distance difference: 14.0000\n",
      "Iteration 200, Loss: 1.5048, Accuracy: 0.4333 mean distance difference: 6.6667, Final distance difference: 9.0000\n",
      "Iteration 300, Loss: 1.2540, Accuracy: 0.6000 mean distance difference: 6.4000, Final distance difference: 9.0000\n",
      "Iteration 400, Loss: 1.3918, Accuracy: 0.5000 mean distance difference: 6.0000, Final distance difference: 11.0000\n",
      "Iteration 500, Loss: 1.2434, Accuracy: 0.5250 mean distance difference: 7.7500, Final distance difference: 5.0000\n",
      "Iteration 600, Loss: 1.3322, Accuracy: 0.5000 mean distance difference: 7.3750, Final distance difference: 0.0000\n",
      "Iteration 700, Loss: 1.4689, Accuracy: 0.4400 mean distance difference: 7.8000, Final distance difference: 5.0000\n",
      "Iteration 800, Loss: 1.3596, Accuracy: 0.5000 mean distance difference: 4.5000, Final distance difference: 15.0000\n",
      "Iteration 900, Loss: 1.3148, Accuracy: 0.4000 mean distance difference: 9.8000, Final distance difference: 15.0000\n",
      "Iteration 1000, Loss: 1.3921, Accuracy: 0.4000 mean distance difference: 8.7778, Final distance difference: 14.0000\n",
      "Iteration 1100, Loss: 1.1231, Accuracy: 0.7333 mean distance difference: 4.1667, Final distance difference: 0.0000\n",
      "Iteration 1200, Loss: 1.3358, Accuracy: 0.4571 mean distance difference: 9.8571, Final distance difference: 14.0000\n",
      "Iteration 1300, Loss: 1.1504, Accuracy: 0.6222 mean distance difference: 6.6667, Final distance difference: 15.0000\n",
      "Iteration 1400, Loss: 1.3080, Accuracy: 0.5200 mean distance difference: 6.4000, Final distance difference: 7.0000\n",
      "Iteration 1500, Loss: 1.1927, Accuracy: 0.5333 mean distance difference: 7.6667, Final distance difference: 15.0000\n",
      "Iteration 1600, Loss: 1.0535, Accuracy: 0.6800 mean distance difference: 5.4000, Final distance difference: 0.0000\n",
      "Iteration 1700, Loss: 1.2008, Accuracy: 0.6400 mean distance difference: 6.6000, Final distance difference: 0.0000\n",
      "Iteration 1800, Loss: 1.3123, Accuracy: 0.4571 mean distance difference: 7.2857, Final distance difference: 5.0000\n",
      "Iteration 1900, Loss: 1.0197, Accuracy: 0.6667 mean distance difference: 6.3333, Final distance difference: 12.0000\n",
      "Iteration 2000, Loss: 1.7911, Accuracy: 0.2000 mean distance difference: 12.1667, Final distance difference: 15.0000\n",
      "Iteration 2100, Loss: 1.3952, Accuracy: 0.4250 mean distance difference: 10.3750, Final distance difference: 12.0000\n",
      "Iteration 2200, Loss: 1.4055, Accuracy: 0.6571 mean distance difference: 8.5714, Final distance difference: 21.0000\n",
      "Iteration 2300, Loss: 0.9943, Accuracy: 0.7200 mean distance difference: 3.6000, Final distance difference: 7.0000\n",
      "Iteration 2400, Loss: 1.2816, Accuracy: 0.5600 mean distance difference: 6.0000, Final distance difference: 8.0000\n",
      "Iteration 2500, Loss: 0.9041, Accuracy: 0.6000 mean distance difference: 6.6000, Final distance difference: 0.0000\n",
      "Iteration 2600, Loss: 1.3182, Accuracy: 0.6000 mean distance difference: 6.6000, Final distance difference: 3.0000\n",
      "Iteration 2700, Loss: 1.6764, Accuracy: 0.1200 mean distance difference: 16.2000, Final distance difference: 14.0000\n",
      "Iteration 2800, Loss: 2.1011, Accuracy: 0.0000 mean distance difference: 13.1111, Final distance difference: 15.0000\n",
      "Iteration 2900, Loss: 0.9603, Accuracy: 0.6500 mean distance difference: 4.3750, Final distance difference: 9.0000\n",
      "Iteration 3000, Loss: 0.8418, Accuracy: 0.7500 mean distance difference: 4.6250, Final distance difference: 9.0000\n",
      "Iteration 3100, Loss: 1.5780, Accuracy: 0.0800 mean distance difference: 23.4000, Final distance difference: 30.0000\n",
      "Iteration 3200, Loss: 1.5049, Accuracy: 0.4857 mean distance difference: 14.1429, Final distance difference: 15.0000\n",
      "Iteration 3300, Loss: 1.4348, Accuracy: 0.6000 mean distance difference: 6.0000, Final distance difference: 5.0000\n",
      "Iteration 3400, Loss: 1.2987, Accuracy: 0.5200 mean distance difference: 6.0000, Final distance difference: 6.0000\n",
      "Iteration 3500, Loss: 0.8359, Accuracy: 0.8800 mean distance difference: 5.0000, Final distance difference: 5.0000\n",
      "Iteration 3600, Loss: 1.7319, Accuracy: 0.1000 mean distance difference: 21.3750, Final distance difference: 23.0000\n",
      "Iteration 3700, Loss: 1.1633, Accuracy: 0.5250 mean distance difference: 7.0000, Final distance difference: 21.0000\n",
      "Iteration 3800, Loss: 0.9267, Accuracy: 0.6889 mean distance difference: 6.2222, Final distance difference: 0.0000\n",
      "Iteration 3900, Loss: 0.8719, Accuracy: 0.7000 mean distance difference: 6.3333, Final distance difference: 5.0000\n",
      "Iteration 4000, Loss: 0.5544, Accuracy: 0.9600 mean distance difference: 0.2000, Final distance difference: 0.0000\n",
      "Iteration 4100, Loss: 0.9086, Accuracy: 0.6000 mean distance difference: 9.3333, Final distance difference: 5.0000\n",
      "Iteration 4200, Loss: 1.2849, Accuracy: 0.5333 mean distance difference: 7.8333, Final distance difference: 0.0000\n",
      "Iteration 4300, Loss: 1.0971, Accuracy: 0.6750 mean distance difference: 4.7500, Final distance difference: 7.0000\n",
      "Iteration 4400, Loss: 0.6362, Accuracy: 0.7333 mean distance difference: 5.5556, Final distance difference: 3.0000\n",
      "Iteration 4500, Loss: 1.1532, Accuracy: 0.6889 mean distance difference: 5.5556, Final distance difference: 4.0000\n",
      "Iteration 4600, Loss: 0.7008, Accuracy: 0.7750 mean distance difference: 3.3750, Final distance difference: 0.0000\n",
      "Iteration 4700, Loss: 1.1271, Accuracy: 0.3143 mean distance difference: 13.8571, Final distance difference: 20.0000\n",
      "Iteration 4800, Loss: 1.0396, Accuracy: 0.6000 mean distance difference: 7.1667, Final distance difference: 4.0000\n",
      "Iteration 4900, Loss: 0.8620, Accuracy: 0.6286 mean distance difference: 6.5714, Final distance difference: 10.0000\n",
      "Iteration 5000, Loss: 0.9694, Accuracy: 0.7000 mean distance difference: 5.6667, Final distance difference: 7.0000\n",
      "Iteration 5100, Loss: 0.6345, Accuracy: 0.8800 mean distance difference: 2.0000, Final distance difference: 6.0000\n",
      "Iteration 5200, Loss: 0.9097, Accuracy: 0.7333 mean distance difference: 5.3333, Final distance difference: 1.0000\n",
      "Iteration 5300, Loss: 1.4312, Accuracy: 0.6000 mean distance difference: 8.0000, Final distance difference: 17.0000\n",
      "Iteration 5400, Loss: 0.5743, Accuracy: 0.8571 mean distance difference: 2.7143, Final distance difference: 0.0000\n",
      "Iteration 5500, Loss: 0.8734, Accuracy: 0.8400 mean distance difference: 3.2000, Final distance difference: 11.0000\n",
      "Iteration 5600, Loss: 0.7822, Accuracy: 0.9000 mean distance difference: 2.3333, Final distance difference: 3.0000\n",
      "Iteration 5700, Loss: 0.6094, Accuracy: 0.9111 mean distance difference: 1.6667, Final distance difference: 0.0000\n",
      "Iteration 5800, Loss: 0.9177, Accuracy: 0.3200 mean distance difference: 10.6000, Final distance difference: 15.0000\n",
      "Iteration 5900, Loss: 0.8211, Accuracy: 0.6800 mean distance difference: 6.4000, Final distance difference: 8.0000\n",
      "Iteration 6000, Loss: 0.9440, Accuracy: 0.7143 mean distance difference: 4.1429, Final distance difference: 9.0000\n",
      "Iteration 6100, Loss: 1.1785, Accuracy: 0.5667 mean distance difference: 7.8333, Final distance difference: 12.0000\n",
      "Iteration 6200, Loss: 1.3474, Accuracy: 0.2889 mean distance difference: 13.4444, Final distance difference: 21.0000\n",
      "Iteration 6300, Loss: 0.5836, Accuracy: 0.8444 mean distance difference: 2.6667, Final distance difference: 7.0000\n",
      "Iteration 6400, Loss: 0.6753, Accuracy: 0.8000 mean distance difference: 3.1429, Final distance difference: 7.0000\n",
      "Iteration 6500, Loss: 0.5430, Accuracy: 0.9111 mean distance difference: 1.4444, Final distance difference: 0.0000\n",
      "Iteration 6600, Loss: 0.8046, Accuracy: 0.6400 mean distance difference: 9.8000, Final distance difference: 12.0000\n",
      "Iteration 6700, Loss: 0.9011, Accuracy: 0.6250 mean distance difference: 8.5000, Final distance difference: 0.0000\n",
      "Iteration 6800, Loss: 1.1383, Accuracy: 0.7111 mean distance difference: 4.4444, Final distance difference: 7.0000\n",
      "Iteration 6900, Loss: 0.4291, Accuracy: 0.9200 mean distance difference: 1.4000, Final distance difference: 0.0000\n",
      "Iteration 7000, Loss: 0.6417, Accuracy: 0.7143 mean distance difference: 4.8571, Final distance difference: 6.0000\n",
      "Iteration 7100, Loss: 0.7890, Accuracy: 0.7500 mean distance difference: 5.7500, Final distance difference: 12.0000\n",
      "Iteration 7200, Loss: 0.8927, Accuracy: 0.5500 mean distance difference: 6.3750, Final distance difference: 14.0000\n",
      "Iteration 7300, Loss: 1.1240, Accuracy: 0.4800 mean distance difference: 6.6000, Final distance difference: 6.0000\n",
      "Iteration 7400, Loss: 1.0524, Accuracy: 0.4889 mean distance difference: 7.5556, Final distance difference: 0.0000\n",
      "Iteration 7500, Loss: 0.5185, Accuracy: 0.8250 mean distance difference: 3.1250, Final distance difference: 3.0000\n",
      "Iteration 7600, Loss: 0.3340, Accuracy: 0.9000 mean distance difference: 2.6667, Final distance difference: 0.0000\n",
      "Iteration 7700, Loss: 0.9303, Accuracy: 0.6667 mean distance difference: 7.1667, Final distance difference: 14.0000\n",
      "Iteration 7800, Loss: 1.1201, Accuracy: 0.4444 mean distance difference: 6.8889, Final distance difference: 1.0000\n",
      "Iteration 7900, Loss: 1.1397, Accuracy: 0.5600 mean distance difference: 10.4000, Final distance difference: 5.0000\n",
      "Iteration 8000, Loss: 0.6713, Accuracy: 0.7429 mean distance difference: 5.1429, Final distance difference: 12.0000\n",
      "Iteration 8100, Loss: 0.6500, Accuracy: 0.8286 mean distance difference: 3.2857, Final distance difference: 0.0000\n",
      "Iteration 8200, Loss: 0.9164, Accuracy: 0.7200 mean distance difference: 4.4000, Final distance difference: 0.0000\n",
      "Iteration 8300, Loss: 0.8199, Accuracy: 0.7200 mean distance difference: 5.8000, Final distance difference: 10.0000\n",
      "Iteration 8400, Loss: 0.8471, Accuracy: 0.7250 mean distance difference: 4.1250, Final distance difference: 0.0000\n",
      "Iteration 8500, Loss: 0.4880, Accuracy: 0.9000 mean distance difference: 2.5000, Final distance difference: 5.0000\n",
      "Iteration 8600, Loss: 1.0268, Accuracy: 0.4800 mean distance difference: 10.4000, Final distance difference: 4.0000\n",
      "Iteration 8700, Loss: 0.4328, Accuracy: 0.8250 mean distance difference: 2.7500, Final distance difference: 0.0000\n",
      "Iteration 8800, Loss: 0.4376, Accuracy: 0.8667 mean distance difference: 3.0000, Final distance difference: 4.0000\n",
      "Iteration 8900, Loss: 1.0693, Accuracy: 0.5333 mean distance difference: 10.1667, Final distance difference: 10.0000\n",
      "Iteration 9000, Loss: 0.8111, Accuracy: 0.6444 mean distance difference: 7.4444, Final distance difference: 7.0000\n",
      "Iteration 9100, Loss: 0.6411, Accuracy: 0.7333 mean distance difference: 3.6667, Final distance difference: 4.0000\n",
      "Iteration 9200, Loss: 0.9125, Accuracy: 0.3143 mean distance difference: 9.4286, Final distance difference: 3.0000\n",
      "Iteration 9300, Loss: 0.6057, Accuracy: 0.8000 mean distance difference: 3.8000, Final distance difference: 0.0000\n",
      "Iteration 9400, Loss: 0.7008, Accuracy: 0.8571 mean distance difference: 2.0000, Final distance difference: 5.0000\n",
      "Iteration 9500, Loss: 0.5413, Accuracy: 0.8667 mean distance difference: 3.0000, Final distance difference: 0.0000\n",
      "Iteration 9600, Loss: 0.7021, Accuracy: 0.8800 mean distance difference: 1.4000, Final distance difference: 0.0000\n",
      "Iteration 9700, Loss: 0.6225, Accuracy: 0.8286 mean distance difference: 3.1429, Final distance difference: 0.0000\n",
      "Iteration 9800, Loss: 0.6345, Accuracy: 0.8000 mean distance difference: 2.3333, Final distance difference: 5.0000\n",
      "Iteration 9900, Loss: 1.3036, Accuracy: 0.4000 mean distance difference: 9.6000, Final distance difference: 6.0000\n",
      "Iteration 10000, Loss: 0.5995, Accuracy: 0.6857 mean distance difference: 5.8571, Final distance difference: 1.0000\n",
      "Iteration 10100, Loss: 0.7466, Accuracy: 0.7714 mean distance difference: 4.1429, Final distance difference: 2.0000\n",
      "Iteration 10200, Loss: 0.9485, Accuracy: 0.7600 mean distance difference: 5.8000, Final distance difference: 3.0000\n",
      "Iteration 10300, Loss: 0.6383, Accuracy: 0.8222 mean distance difference: 3.8889, Final distance difference: 0.0000\n",
      "Iteration 10400, Loss: 0.5825, Accuracy: 0.8400 mean distance difference: 4.4000, Final distance difference: 5.0000\n",
      "Iteration 10500, Loss: 0.8591, Accuracy: 0.7250 mean distance difference: 3.5000, Final distance difference: 2.0000\n",
      "Iteration 10600, Loss: 1.1808, Accuracy: 0.6222 mean distance difference: 6.3333, Final distance difference: 9.0000\n",
      "Iteration 10700, Loss: 0.5970, Accuracy: 0.8286 mean distance difference: 3.5714, Final distance difference: 0.0000\n",
      "Iteration 10800, Loss: 0.6873, Accuracy: 0.7143 mean distance difference: 4.8571, Final distance difference: 0.0000\n",
      "Iteration 10900, Loss: 0.8025, Accuracy: 0.7000 mean distance difference: 3.3750, Final distance difference: 5.0000\n",
      "Iteration 11000, Loss: 0.5230, Accuracy: 0.9333 mean distance difference: 2.0000, Final distance difference: 0.0000\n",
      "Iteration 11100, Loss: 0.8022, Accuracy: 0.8400 mean distance difference: 4.6000, Final distance difference: 5.0000\n",
      "Iteration 11200, Loss: 1.0335, Accuracy: 0.4857 mean distance difference: 8.4286, Final distance difference: 10.0000\n",
      "Iteration 11300, Loss: 0.5638, Accuracy: 0.9000 mean distance difference: 1.5000, Final distance difference: 0.0000\n",
      "Iteration 11400, Loss: 0.6185, Accuracy: 0.8000 mean distance difference: 4.6250, Final distance difference: 0.0000\n",
      "Iteration 11500, Loss: 0.7675, Accuracy: 0.6000 mean distance difference: 4.5714, Final distance difference: 9.0000\n",
      "Iteration 11600, Loss: 1.1785, Accuracy: 0.4250 mean distance difference: 10.7500, Final distance difference: 22.0000\n",
      "Iteration 11700, Loss: 0.6199, Accuracy: 0.8333 mean distance difference: 3.5000, Final distance difference: 2.0000\n",
      "Iteration 11800, Loss: 0.7854, Accuracy: 0.4000 mean distance difference: 7.5714, Final distance difference: 15.0000\n",
      "Iteration 11900, Loss: 0.7620, Accuracy: 0.7200 mean distance difference: 2.2000, Final distance difference: 3.0000\n",
      "Iteration 12000, Loss: 1.0599, Accuracy: 0.6857 mean distance difference: 7.7143, Final distance difference: 5.0000\n",
      "Iteration 12100, Loss: 0.6420, Accuracy: 0.7714 mean distance difference: 4.0000, Final distance difference: 0.0000\n",
      "Iteration 12200, Loss: 0.5958, Accuracy: 0.8222 mean distance difference: 3.4444, Final distance difference: 5.0000\n",
      "Iteration 12300, Loss: 0.4064, Accuracy: 0.9000 mean distance difference: 1.2500, Final distance difference: 1.0000\n",
      "Iteration 12400, Loss: 1.4499, Accuracy: 0.3429 mean distance difference: 9.5714, Final distance difference: 24.0000\n",
      "Iteration 12500, Loss: 0.5186, Accuracy: 0.8000 mean distance difference: 2.6667, Final distance difference: 5.0000\n",
      "Iteration 12600, Loss: 0.7502, Accuracy: 0.7600 mean distance difference: 4.6000, Final distance difference: 7.0000\n",
      "Iteration 12700, Loss: 0.4169, Accuracy: 0.8571 mean distance difference: 1.8571, Final distance difference: 0.0000\n",
      "Iteration 12800, Loss: 0.4246, Accuracy: 0.8889 mean distance difference: 1.8889, Final distance difference: 0.0000\n",
      "Iteration 12900, Loss: 0.8338, Accuracy: 0.7333 mean distance difference: 7.8333, Final distance difference: 0.0000\n",
      "Iteration 13000, Loss: 0.5674, Accuracy: 0.8500 mean distance difference: 2.6250, Final distance difference: 0.0000\n",
      "Iteration 13100, Loss: 0.6814, Accuracy: 0.8000 mean distance difference: 3.5000, Final distance difference: 2.0000\n",
      "Iteration 13200, Loss: 0.5267, Accuracy: 0.8222 mean distance difference: 2.4444, Final distance difference: 2.0000\n",
      "Iteration 13300, Loss: 0.5048, Accuracy: 0.7778 mean distance difference: 5.8889, Final distance difference: 0.0000\n",
      "Iteration 13400, Loss: 1.0053, Accuracy: 0.6400 mean distance difference: 6.0000, Final distance difference: 4.0000\n",
      "Iteration 13500, Loss: 0.4497, Accuracy: 0.9429 mean distance difference: 1.4286, Final distance difference: 0.0000\n",
      "Iteration 13600, Loss: 1.6471, Accuracy: 0.3429 mean distance difference: 12.1429, Final distance difference: 15.0000\n",
      "Iteration 13700, Loss: 0.5775, Accuracy: 0.8000 mean distance difference: 5.4000, Final distance difference: 10.0000\n",
      "Iteration 13800, Loss: 1.0752, Accuracy: 0.5333 mean distance difference: 6.6667, Final distance difference: 9.0000\n",
      "Iteration 13900, Loss: 0.6313, Accuracy: 0.8444 mean distance difference: 1.8889, Final distance difference: 5.0000\n",
      "Iteration 14000, Loss: 0.6076, Accuracy: 0.7750 mean distance difference: 3.2500, Final distance difference: 0.0000\n",
      "Iteration 14100, Loss: 0.7802, Accuracy: 0.8000 mean distance difference: 4.0000, Final distance difference: 5.0000\n",
      "Iteration 14200, Loss: 0.4310, Accuracy: 0.8667 mean distance difference: 1.5000, Final distance difference: 0.0000\n",
      "Iteration 14300, Loss: 0.5376, Accuracy: 0.8000 mean distance difference: 4.0000, Final distance difference: 5.0000\n",
      "Iteration 14400, Loss: 0.4126, Accuracy: 0.9000 mean distance difference: 1.8750, Final distance difference: 0.0000\n",
      "Iteration 14500, Loss: 0.5654, Accuracy: 0.8000 mean distance difference: 3.2000, Final distance difference: 5.0000\n",
      "Iteration 14600, Loss: 0.8032, Accuracy: 0.8000 mean distance difference: 3.7500, Final distance difference: 3.0000\n",
      "Iteration 14700, Loss: 0.8302, Accuracy: 0.7143 mean distance difference: 5.1429, Final distance difference: 8.0000\n",
      "Iteration 14800, Loss: 0.6185, Accuracy: 0.7600 mean distance difference: 3.2000, Final distance difference: 1.0000\n",
      "Iteration 14900, Loss: 0.5304, Accuracy: 0.8250 mean distance difference: 3.0000, Final distance difference: 5.0000\n",
      "Iteration 15000, Loss: 0.3799, Accuracy: 0.9143 mean distance difference: 1.7143, Final distance difference: 0.0000\n",
      "Iteration 15100, Loss: 0.9888, Accuracy: 0.7333 mean distance difference: 6.6667, Final distance difference: 6.0000\n",
      "Iteration 15200, Loss: 0.8815, Accuracy: 0.7143 mean distance difference: 4.4286, Final distance difference: 6.0000\n",
      "Iteration 15300, Loss: 0.5639, Accuracy: 0.9000 mean distance difference: 2.0000, Final distance difference: 0.0000\n",
      "Iteration 15400, Loss: 0.8523, Accuracy: 0.8333 mean distance difference: 3.6667, Final distance difference: 6.0000\n",
      "Iteration 15500, Loss: 0.7592, Accuracy: 0.8000 mean distance difference: 4.2500, Final distance difference: 5.0000\n",
      "Iteration 15600, Loss: 0.7364, Accuracy: 0.7556 mean distance difference: 5.0000, Final distance difference: 10.0000\n",
      "Iteration 15700, Loss: 0.5408, Accuracy: 0.8750 mean distance difference: 2.0000, Final distance difference: 1.0000\n",
      "Iteration 15800, Loss: 0.3987, Accuracy: 0.9000 mean distance difference: 1.0000, Final distance difference: 0.0000\n",
      "Iteration 15900, Loss: 0.6108, Accuracy: 0.8333 mean distance difference: 3.3333, Final distance difference: 0.0000\n",
      "Iteration 16000, Loss: 0.4735, Accuracy: 0.9000 mean distance difference: 2.0000, Final distance difference: 0.0000\n",
      "Iteration 16100, Loss: 0.6421, Accuracy: 0.7667 mean distance difference: 3.8333, Final distance difference: 0.0000\n",
      "Iteration 16200, Loss: 0.7917, Accuracy: 0.7667 mean distance difference: 4.5000, Final distance difference: 5.0000\n",
      "Iteration 16300, Loss: 0.6417, Accuracy: 0.8250 mean distance difference: 3.3750, Final distance difference: 4.0000\n",
      "Iteration 16400, Loss: 0.7253, Accuracy: 0.7667 mean distance difference: 4.6667, Final distance difference: 9.0000\n",
      "Iteration 16500, Loss: 0.9111, Accuracy: 0.7429 mean distance difference: 5.0000, Final distance difference: 3.0000\n",
      "Iteration 16600, Loss: 0.8529, Accuracy: 0.7333 mean distance difference: 5.7778, Final distance difference: 7.0000\n",
      "Iteration 16700, Loss: 0.6434, Accuracy: 0.8000 mean distance difference: 2.4286, Final distance difference: 3.0000\n",
      "Iteration 16800, Loss: 0.8027, Accuracy: 0.8571 mean distance difference: 3.2857, Final distance difference: 0.0000\n",
      "Iteration 16900, Loss: 0.9067, Accuracy: 0.7600 mean distance difference: 5.2000, Final distance difference: 5.0000\n",
      "Iteration 17000, Loss: 0.8200, Accuracy: 0.7333 mean distance difference: 4.8333, Final distance difference: 3.0000\n",
      "Iteration 17100, Loss: 0.6101, Accuracy: 0.8444 mean distance difference: 3.4444, Final distance difference: 0.0000\n",
      "Iteration 17200, Loss: 0.9673, Accuracy: 0.6286 mean distance difference: 4.2857, Final distance difference: 4.0000\n",
      "Iteration 17300, Loss: 0.9428, Accuracy: 0.6333 mean distance difference: 4.3333, Final distance difference: 3.0000\n",
      "Iteration 17400, Loss: 1.0544, Accuracy: 0.3429 mean distance difference: 10.5714, Final distance difference: 16.0000\n",
      "Iteration 17500, Loss: 0.5261, Accuracy: 0.8857 mean distance difference: 1.2857, Final distance difference: 3.0000\n",
      "Iteration 17600, Loss: 0.9437, Accuracy: 0.6857 mean distance difference: 6.7143, Final distance difference: 8.0000\n",
      "Iteration 17700, Loss: 0.4574, Accuracy: 0.9000 mean distance difference: 2.0000, Final distance difference: 0.0000\n",
      "Iteration 17800, Loss: 0.8501, Accuracy: 0.7600 mean distance difference: 4.2000, Final distance difference: 0.0000\n",
      "Iteration 17900, Loss: 0.4459, Accuracy: 0.9200 mean distance difference: 2.2000, Final distance difference: 10.0000\n",
      "Iteration 18000, Loss: 0.8551, Accuracy: 0.7250 mean distance difference: 5.5000, Final distance difference: 7.0000\n",
      "Iteration 18100, Loss: 0.7443, Accuracy: 0.7778 mean distance difference: 4.8889, Final distance difference: 0.0000\n",
      "Iteration 18200, Loss: 0.5130, Accuracy: 0.8571 mean distance difference: 2.1429, Final distance difference: 5.0000\n",
      "Iteration 18300, Loss: 0.4547, Accuracy: 0.9111 mean distance difference: 1.4444, Final distance difference: 0.0000\n",
      "Iteration 18400, Loss: 0.3812, Accuracy: 0.8800 mean distance difference: 2.0000, Final distance difference: 5.0000\n",
      "Iteration 18500, Loss: 0.5080, Accuracy: 0.8500 mean distance difference: 2.7500, Final distance difference: 0.0000\n",
      "Iteration 18600, Loss: 0.4633, Accuracy: 0.8889 mean distance difference: 1.7778, Final distance difference: 0.0000\n",
      "Iteration 18700, Loss: 0.4736, Accuracy: 0.8400 mean distance difference: 2.4000, Final distance difference: 0.0000\n",
      "Iteration 18800, Loss: 0.4072, Accuracy: 0.8333 mean distance difference: 4.3333, Final distance difference: 0.0000\n",
      "Iteration 18900, Loss: 0.4063, Accuracy: 0.9143 mean distance difference: 1.2857, Final distance difference: 2.0000\n",
      "Iteration 19000, Loss: 0.3989, Accuracy: 0.8571 mean distance difference: 1.5714, Final distance difference: 1.0000\n",
      "Iteration 19100, Loss: 0.5685, Accuracy: 0.8222 mean distance difference: 2.7778, Final distance difference: 0.0000\n",
      "Iteration 19200, Loss: 0.6346, Accuracy: 0.8444 mean distance difference: 4.0000, Final distance difference: 5.0000\n",
      "Iteration 19300, Loss: 0.6311, Accuracy: 0.8571 mean distance difference: 2.8571, Final distance difference: 0.0000\n",
      "Iteration 19400, Loss: 0.4940, Accuracy: 0.8333 mean distance difference: 3.0000, Final distance difference: 0.0000\n",
      "Iteration 19500, Loss: 0.5562, Accuracy: 0.8571 mean distance difference: 2.2857, Final distance difference: 0.0000\n",
      "Iteration 19600, Loss: 0.5409, Accuracy: 0.8750 mean distance difference: 2.1250, Final distance difference: 0.0000\n",
      "Iteration 19700, Loss: 0.6862, Accuracy: 0.8333 mean distance difference: 2.1667, Final distance difference: 0.0000\n",
      "Iteration 19800, Loss: 0.5506, Accuracy: 0.7556 mean distance difference: 3.3333, Final distance difference: 0.0000\n",
      "Iteration 19900, Loss: 0.5085, Accuracy: 0.8500 mean distance difference: 3.3750, Final distance difference: 0.0000\n",
      "Iteration 20000, Loss: 0.9816, Accuracy: 0.7000 mean distance difference: 4.7500, Final distance difference: 0.0000\n",
      "Iteration 20100, Loss: 0.5652, Accuracy: 0.9250 mean distance difference: 1.5000, Final distance difference: 2.0000\n",
      "Iteration 20200, Loss: 0.7110, Accuracy: 0.7714 mean distance difference: 4.7143, Final distance difference: 5.0000\n",
      "Iteration 20300, Loss: 0.6194, Accuracy: 0.8222 mean distance difference: 3.1111, Final distance difference: 5.0000\n",
      "Iteration 20400, Loss: 0.3764, Accuracy: 0.8571 mean distance difference: 2.7143, Final distance difference: 0.0000\n",
      "Iteration 20500, Loss: 1.3568, Accuracy: 0.3333 mean distance difference: 12.5000, Final distance difference: 10.0000\n",
      "Iteration 20600, Loss: 1.0519, Accuracy: 0.6667 mean distance difference: 5.3333, Final distance difference: 1.0000\n",
      "Iteration 20700, Loss: 0.4238, Accuracy: 0.8400 mean distance difference: 2.0000, Final distance difference: 0.0000\n",
      "Iteration 20800, Loss: 0.7348, Accuracy: 0.7429 mean distance difference: 4.8571, Final distance difference: 0.0000\n",
      "Iteration 20900, Loss: 0.5836, Accuracy: 0.8667 mean distance difference: 2.5000, Final distance difference: 0.0000\n",
      "Iteration 21000, Loss: 0.5868, Accuracy: 0.8667 mean distance difference: 1.6667, Final distance difference: 0.0000\n",
      "Iteration 21100, Loss: 0.4763, Accuracy: 0.8889 mean distance difference: 1.3333, Final distance difference: 0.0000\n",
      "Iteration 21200, Loss: 0.5386, Accuracy: 0.8667 mean distance difference: 3.0000, Final distance difference: 6.0000\n",
      "Iteration 21300, Loss: 0.7242, Accuracy: 0.8000 mean distance difference: 2.5000, Final distance difference: 1.0000\n",
      "Iteration 21400, Loss: 0.9907, Accuracy: 0.7000 mean distance difference: 4.5000, Final distance difference: 2.0000\n",
      "Iteration 21500, Loss: 0.7866, Accuracy: 0.7600 mean distance difference: 6.2000, Final distance difference: 5.0000\n",
      "Iteration 21600, Loss: 0.3387, Accuracy: 0.9000 mean distance difference: 1.0000, Final distance difference: 2.0000\n",
      "Iteration 21700, Loss: 0.5990, Accuracy: 0.8000 mean distance difference: 4.3333, Final distance difference: 9.0000\n",
      "Iteration 21800, Loss: 0.5367, Accuracy: 0.9111 mean distance difference: 2.4444, Final distance difference: 0.0000\n",
      "Iteration 21900, Loss: 0.6233, Accuracy: 0.8250 mean distance difference: 2.6250, Final distance difference: 3.0000\n",
      "Iteration 22000, Loss: 0.8780, Accuracy: 0.6571 mean distance difference: 5.5714, Final distance difference: 4.0000\n",
      "Iteration 22100, Loss: 0.4053, Accuracy: 0.9000 mean distance difference: 1.8333, Final distance difference: 0.0000\n",
      "Iteration 22200, Loss: 0.8026, Accuracy: 0.6800 mean distance difference: 5.8000, Final distance difference: 2.0000\n",
      "Iteration 22300, Loss: 0.6236, Accuracy: 0.7750 mean distance difference: 4.0000, Final distance difference: 3.0000\n",
      "Iteration 22400, Loss: 0.9597, Accuracy: 0.7333 mean distance difference: 6.3333, Final distance difference: 0.0000\n",
      "Iteration 22500, Loss: 0.8490, Accuracy: 0.7143 mean distance difference: 4.8571, Final distance difference: 6.0000\n",
      "Iteration 22600, Loss: 0.5694, Accuracy: 0.8000 mean distance difference: 3.5556, Final distance difference: 9.0000\n",
      "Iteration 22700, Loss: 0.5449, Accuracy: 0.8750 mean distance difference: 1.5000, Final distance difference: 5.0000\n",
      "Iteration 22800, Loss: 0.7459, Accuracy: 0.8000 mean distance difference: 2.8333, Final distance difference: 7.0000\n",
      "Iteration 22900, Loss: 0.8675, Accuracy: 0.7500 mean distance difference: 3.5000, Final distance difference: 0.0000\n",
      "Iteration 23000, Loss: 1.0707, Accuracy: 0.5143 mean distance difference: 7.4286, Final distance difference: 3.0000\n",
      "Iteration 23100, Loss: 0.4681, Accuracy: 0.8000 mean distance difference: 2.4000, Final distance difference: 1.0000\n",
      "Iteration 23200, Loss: 0.9072, Accuracy: 0.6400 mean distance difference: 7.0000, Final distance difference: 9.0000\n",
      "Iteration 23300, Loss: 0.5401, Accuracy: 0.8571 mean distance difference: 2.7143, Final distance difference: 0.0000\n",
      "Iteration 23400, Loss: 0.3841, Accuracy: 0.8667 mean distance difference: 2.3333, Final distance difference: 5.0000\n",
      "Iteration 23500, Loss: 0.3106, Accuracy: 0.9500 mean distance difference: 0.8750, Final distance difference: 3.0000\n",
      "Iteration 23600, Loss: 0.7668, Accuracy: 0.6500 mean distance difference: 9.0000, Final distance difference: 0.0000\n",
      "Iteration 23700, Loss: 0.9849, Accuracy: 0.3500 mean distance difference: 9.6250, Final distance difference: 15.0000\n",
      "Iteration 23800, Loss: 0.8329, Accuracy: 0.6750 mean distance difference: 5.6250, Final distance difference: 7.0000\n",
      "Iteration 23900, Loss: 0.3480, Accuracy: 0.9200 mean distance difference: 0.6000, Final distance difference: 0.0000\n",
      "Iteration 24000, Loss: 0.3772, Accuracy: 0.8857 mean distance difference: 2.7143, Final distance difference: 8.0000\n",
      "Iteration 24100, Loss: 0.7030, Accuracy: 0.8750 mean distance difference: 2.1250, Final distance difference: 0.0000\n",
      "Iteration 24200, Loss: 0.7831, Accuracy: 0.6333 mean distance difference: 4.6667, Final distance difference: 5.0000\n",
      "Iteration 24300, Loss: 0.5810, Accuracy: 0.8000 mean distance difference: 2.7500, Final distance difference: 6.0000\n",
      "Iteration 24400, Loss: 0.8206, Accuracy: 0.7000 mean distance difference: 5.3750, Final distance difference: 5.0000\n",
      "Iteration 24500, Loss: 1.1951, Accuracy: 0.6000 mean distance difference: 4.0000, Final distance difference: 0.0000\n",
      "Iteration 24600, Loss: 0.3920, Accuracy: 0.9429 mean distance difference: 1.0000, Final distance difference: 0.0000\n",
      "Iteration 24700, Loss: 0.4927, Accuracy: 0.8800 mean distance difference: 2.4000, Final distance difference: 6.0000\n",
      "Iteration 24800, Loss: 0.4398, Accuracy: 0.8857 mean distance difference: 2.1429, Final distance difference: 0.0000\n",
      "Iteration 24900, Loss: 0.7721, Accuracy: 0.8571 mean distance difference: 1.4286, Final distance difference: 4.0000\n",
      "Iteration 25000, Loss: 0.7077, Accuracy: 0.7250 mean distance difference: 4.0000, Final distance difference: 6.0000\n",
      "Iteration 25100, Loss: 0.7312, Accuracy: 0.8250 mean distance difference: 3.1250, Final distance difference: 0.0000\n",
      "Iteration 25200, Loss: 0.3740, Accuracy: 0.8857 mean distance difference: 1.2857, Final distance difference: 0.0000\n",
      "Iteration 25300, Loss: 0.7736, Accuracy: 0.8500 mean distance difference: 3.0000, Final distance difference: 0.0000\n",
      "Iteration 25400, Loss: 0.4618, Accuracy: 0.8800 mean distance difference: 1.8000, Final distance difference: 0.0000\n",
      "Iteration 25500, Loss: 0.3847, Accuracy: 0.8889 mean distance difference: 1.8889, Final distance difference: 4.0000\n",
      "Iteration 25600, Loss: 0.4871, Accuracy: 0.8800 mean distance difference: 2.8000, Final distance difference: 0.0000\n",
      "Iteration 25700, Loss: 0.6115, Accuracy: 0.8000 mean distance difference: 3.1111, Final distance difference: 9.0000\n",
      "Iteration 25800, Loss: 0.3015, Accuracy: 0.9333 mean distance difference: 0.8333, Final distance difference: 2.0000\n",
      "Iteration 25900, Loss: 0.4690, Accuracy: 0.8400 mean distance difference: 4.8000, Final distance difference: 6.0000\n",
      "Iteration 26000, Loss: 0.7343, Accuracy: 0.8000 mean distance difference: 3.5000, Final distance difference: 0.0000\n",
      "Iteration 26100, Loss: 0.5900, Accuracy: 0.8667 mean distance difference: 1.1667, Final distance difference: 1.0000\n",
      "Iteration 26200, Loss: 0.7559, Accuracy: 0.7600 mean distance difference: 4.4000, Final distance difference: 8.0000\n",
      "Iteration 26300, Loss: 0.4222, Accuracy: 0.9143 mean distance difference: 1.2857, Final distance difference: 3.0000\n",
      "Iteration 26400, Loss: 0.7473, Accuracy: 0.7667 mean distance difference: 3.5000, Final distance difference: 5.0000\n",
      "Iteration 26500, Loss: 1.0202, Accuracy: 0.4857 mean distance difference: 10.7143, Final distance difference: 19.0000\n",
      "Iteration 26600, Loss: 0.7820, Accuracy: 0.7333 mean distance difference: 5.0000, Final distance difference: 0.0000\n",
      "Iteration 26700, Loss: 0.6492, Accuracy: 0.8000 mean distance difference: 3.1429, Final distance difference: 1.0000\n",
      "Iteration 26800, Loss: 0.9659, Accuracy: 0.6800 mean distance difference: 4.0000, Final distance difference: 9.0000\n",
      "Iteration 26900, Loss: 1.0662, Accuracy: 0.6750 mean distance difference: 5.1250, Final distance difference: 10.0000\n",
      "Iteration 27000, Loss: 0.5863, Accuracy: 0.8667 mean distance difference: 2.6667, Final distance difference: 8.0000\n",
      "Iteration 27100, Loss: 0.8657, Accuracy: 0.6000 mean distance difference: 6.8000, Final distance difference: 21.0000\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[31], line 17\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m training:\n\u001b[1;32m     15\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m50000\u001b[39m):\n\u001b[1;32m     16\u001b[0m         \u001b[38;5;66;03m#(samples, batch_size, dim, n_points)\u001b[39;00m\n\u001b[0;32m---> 17\u001b[0m         xc, yc, xt, yt, _ \u001b[38;5;241m=\u001b[39m \u001b[43mget_batch\u001b[49m\u001b[43m(\u001b[49m\u001b[43msampler\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     19\u001b[0m         dist \u001b[38;5;241m=\u001b[39m agnp(xc, yc, xt)\n\u001b[1;32m     21\u001b[0m         logits \u001b[38;5;241m=\u001b[39m dist\u001b[38;5;241m.\u001b[39mmean  \n",
      "Cell \u001b[0;32mIn[13], line 11\u001b[0m, in \u001b[0;36mget_batch\u001b[0;34m(sampler, device)\u001b[0m\n\u001b[1;32m      7\u001b[0m user_params \u001b[38;5;241m=\u001b[39m sampler\u001b[38;5;241m.\u001b[39mgenerate_user_parameters()\n\u001b[1;32m      9\u001b[0m n_trajectories \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mrandom\u001b[38;5;241m.\u001b[39mrandint(low \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m5\u001b[39m, high \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m10\u001b[39m)\n\u001b[0;32m---> 11\u001b[0m trajectories \u001b[38;5;241m=\u001b[39m \u001b[43msampler\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate_user_trajectories\u001b[49m\u001b[43m(\u001b[49m\u001b[43mn_trajectories\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43muser_params\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     13\u001b[0m xc, yc, xt, yt \u001b[38;5;241m=\u001b[39m build_context_and_target(trajectories)\n\u001b[1;32m     15\u001b[0m xc \u001b[38;5;241m=\u001b[39m xc\u001b[38;5;241m.\u001b[39mpermute(\u001b[38;5;241m0\u001b[39m,\u001b[38;5;241m2\u001b[39m,\u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m.\u001b[39mto(device)\n",
      "File \u001b[0;32m~/Aalto/internship2024/pomdp_mcts_gridworld/src/utils/sampler.py:90\u001b[0m, in \u001b[0;36mSampler.generate_user_trajectories\u001b[0;34m(self, num_trajectories, user_params)\u001b[0m\n\u001b[1;32m     88\u001b[0m   action \u001b[38;5;241m=\u001b[39m env\u001b[38;5;241m.\u001b[39maction_space\u001b[38;5;241m.\u001b[39msample()\n\u001b[1;32m     89\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 90\u001b[0m   action \u001b[38;5;241m=\u001b[39m \u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmax_neighboring_reward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     92\u001b[0m action_onehot \u001b[38;5;241m=\u001b[39m [\u001b[38;5;241m0\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(env\u001b[38;5;241m.\u001b[39maction_space\u001b[38;5;241m.\u001b[39mn \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m)]\n\u001b[1;32m     93\u001b[0m action_onehot[action] \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n",
      "File \u001b[0;32m~/Aalto/internship2024/pomdp_mcts_gridworld/src/grid_world.py:66\u001b[0m, in \u001b[0;36mGridWorld.max_neighboring_reward\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     64\u001b[0m best_action \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m     65\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m action \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(Actions)):\n\u001b[0;32m---> 66\u001b[0m     env_copy \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcopy\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     67\u001b[0m     obs, reward, terminated, truncated, _ \u001b[38;5;241m=\u001b[39m env_copy\u001b[38;5;241m.\u001b[39mstep(action)\n\u001b[1;32m     68\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m reward \u001b[38;5;241m>\u001b[39m best_reward:\n",
      "File \u001b[0;32m~/Aalto/internship2024/pomdp_mcts_gridworld/src/grid_world.py:328\u001b[0m, in \u001b[0;36mGridWorld.copy\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    326\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcopy\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    327\u001b[0m     new_env \u001b[38;5;241m=\u001b[39m GridWorld(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mwidth \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m2\u001b[39m, start_pos\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39magent_pos, agent_view_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39magent_view_size)\n\u001b[0;32m--> 328\u001b[0m     new_env\u001b[38;5;241m.\u001b[39mgrid \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgrid\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcopy\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    329\u001b[0m     new_env\u001b[38;5;241m.\u001b[39mstep_count \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstep_count\n\u001b[1;32m    330\u001b[0m     new_env\u001b[38;5;241m.\u001b[39magent_dir \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39magent_dir\n",
      "File \u001b[0;32m~/Aalto/internship2024/pomdp_mcts_gridworld/src/core/grid.py:26\u001b[0m, in \u001b[0;36mModifiedGrid.copy\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcopy\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mModifiedGrid\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[1;32m     24\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mcopy\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m deepcopy\n\u001b[0;32m---> 26\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mdeepcopy\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/pml2/lib/python3.10/copy.py:172\u001b[0m, in \u001b[0;36mdeepcopy\u001b[0;34m(x, memo, _nil)\u001b[0m\n\u001b[1;32m    170\u001b[0m                 y \u001b[38;5;241m=\u001b[39m x\n\u001b[1;32m    171\u001b[0m             \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 172\u001b[0m                 y \u001b[38;5;241m=\u001b[39m \u001b[43m_reconstruct\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmemo\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mrv\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    174\u001b[0m \u001b[38;5;66;03m# If is its own copy, don't memoize.\u001b[39;00m\n\u001b[1;32m    175\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m y \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m x:\n",
      "File \u001b[0;32m~/anaconda3/envs/pml2/lib/python3.10/copy.py:271\u001b[0m, in \u001b[0;36m_reconstruct\u001b[0;34m(x, memo, func, args, state, listiter, dictiter, deepcopy)\u001b[0m\n\u001b[1;32m    269\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m state \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    270\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m deep:\n\u001b[0;32m--> 271\u001b[0m         state \u001b[38;5;241m=\u001b[39m \u001b[43mdeepcopy\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstate\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmemo\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    272\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(y, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m__setstate__\u001b[39m\u001b[38;5;124m'\u001b[39m):\n\u001b[1;32m    273\u001b[0m         y\u001b[38;5;241m.\u001b[39m__setstate__(state)\n",
      "File \u001b[0;32m~/anaconda3/envs/pml2/lib/python3.10/copy.py:146\u001b[0m, in \u001b[0;36mdeepcopy\u001b[0;34m(x, memo, _nil)\u001b[0m\n\u001b[1;32m    144\u001b[0m copier \u001b[38;5;241m=\u001b[39m _deepcopy_dispatch\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;28mcls\u001b[39m)\n\u001b[1;32m    145\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m copier \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 146\u001b[0m     y \u001b[38;5;241m=\u001b[39m \u001b[43mcopier\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmemo\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    147\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    148\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28missubclass\u001b[39m(\u001b[38;5;28mcls\u001b[39m, \u001b[38;5;28mtype\u001b[39m):\n",
      "File \u001b[0;32m~/anaconda3/envs/pml2/lib/python3.10/copy.py:231\u001b[0m, in \u001b[0;36m_deepcopy_dict\u001b[0;34m(x, memo, deepcopy)\u001b[0m\n\u001b[1;32m    229\u001b[0m memo[\u001b[38;5;28mid\u001b[39m(x)] \u001b[38;5;241m=\u001b[39m y\n\u001b[1;32m    230\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m key, value \u001b[38;5;129;01min\u001b[39;00m x\u001b[38;5;241m.\u001b[39mitems():\n\u001b[0;32m--> 231\u001b[0m     y[deepcopy(key, memo)] \u001b[38;5;241m=\u001b[39m \u001b[43mdeepcopy\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmemo\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    232\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m y\n",
      "File \u001b[0;32m~/anaconda3/envs/pml2/lib/python3.10/copy.py:146\u001b[0m, in \u001b[0;36mdeepcopy\u001b[0;34m(x, memo, _nil)\u001b[0m\n\u001b[1;32m    144\u001b[0m copier \u001b[38;5;241m=\u001b[39m _deepcopy_dispatch\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;28mcls\u001b[39m)\n\u001b[1;32m    145\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m copier \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 146\u001b[0m     y \u001b[38;5;241m=\u001b[39m \u001b[43mcopier\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmemo\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    147\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    148\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28missubclass\u001b[39m(\u001b[38;5;28mcls\u001b[39m, \u001b[38;5;28mtype\u001b[39m):\n",
      "File \u001b[0;32m~/anaconda3/envs/pml2/lib/python3.10/copy.py:206\u001b[0m, in \u001b[0;36m_deepcopy_list\u001b[0;34m(x, memo, deepcopy)\u001b[0m\n\u001b[1;32m    204\u001b[0m append \u001b[38;5;241m=\u001b[39m y\u001b[38;5;241m.\u001b[39mappend\n\u001b[1;32m    205\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m a \u001b[38;5;129;01min\u001b[39;00m x:\n\u001b[0;32m--> 206\u001b[0m     append(\u001b[43mdeepcopy\u001b[49m\u001b[43m(\u001b[49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmemo\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m    207\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m y\n",
      "File \u001b[0;32m~/anaconda3/envs/pml2/lib/python3.10/copy.py:172\u001b[0m, in \u001b[0;36mdeepcopy\u001b[0;34m(x, memo, _nil)\u001b[0m\n\u001b[1;32m    170\u001b[0m                 y \u001b[38;5;241m=\u001b[39m x\n\u001b[1;32m    171\u001b[0m             \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 172\u001b[0m                 y \u001b[38;5;241m=\u001b[39m \u001b[43m_reconstruct\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmemo\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mrv\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    174\u001b[0m \u001b[38;5;66;03m# If is its own copy, don't memoize.\u001b[39;00m\n\u001b[1;32m    175\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m y \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m x:\n",
      "File \u001b[0;32m~/anaconda3/envs/pml2/lib/python3.10/copy.py:271\u001b[0m, in \u001b[0;36m_reconstruct\u001b[0;34m(x, memo, func, args, state, listiter, dictiter, deepcopy)\u001b[0m\n\u001b[1;32m    269\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m state \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    270\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m deep:\n\u001b[0;32m--> 271\u001b[0m         state \u001b[38;5;241m=\u001b[39m \u001b[43mdeepcopy\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstate\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmemo\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    272\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(y, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m__setstate__\u001b[39m\u001b[38;5;124m'\u001b[39m):\n\u001b[1;32m    273\u001b[0m         y\u001b[38;5;241m.\u001b[39m__setstate__(state)\n",
      "File \u001b[0;32m~/anaconda3/envs/pml2/lib/python3.10/copy.py:146\u001b[0m, in \u001b[0;36mdeepcopy\u001b[0;34m(x, memo, _nil)\u001b[0m\n\u001b[1;32m    144\u001b[0m copier \u001b[38;5;241m=\u001b[39m _deepcopy_dispatch\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;28mcls\u001b[39m)\n\u001b[1;32m    145\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m copier \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 146\u001b[0m     y \u001b[38;5;241m=\u001b[39m \u001b[43mcopier\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmemo\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    147\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    148\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28missubclass\u001b[39m(\u001b[38;5;28mcls\u001b[39m, \u001b[38;5;28mtype\u001b[39m):\n",
      "File \u001b[0;32m~/anaconda3/envs/pml2/lib/python3.10/copy.py:231\u001b[0m, in \u001b[0;36m_deepcopy_dict\u001b[0;34m(x, memo, deepcopy)\u001b[0m\n\u001b[1;32m    229\u001b[0m memo[\u001b[38;5;28mid\u001b[39m(x)] \u001b[38;5;241m=\u001b[39m y\n\u001b[1;32m    230\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m key, value \u001b[38;5;129;01min\u001b[39;00m x\u001b[38;5;241m.\u001b[39mitems():\n\u001b[0;32m--> 231\u001b[0m     y[deepcopy(key, memo)] \u001b[38;5;241m=\u001b[39m \u001b[43mdeepcopy\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmemo\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    232\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m y\n",
      "File \u001b[0;32m~/anaconda3/envs/pml2/lib/python3.10/copy.py:128\u001b[0m, in \u001b[0;36mdeepcopy\u001b[0;34m(x, memo, _nil)\u001b[0m\n\u001b[1;32m    124\u001b[0m     d[PyStringMap] \u001b[38;5;241m=\u001b[39m PyStringMap\u001b[38;5;241m.\u001b[39mcopy\n\u001b[1;32m    126\u001b[0m \u001b[38;5;28;01mdel\u001b[39;00m d, t\n\u001b[0;32m--> 128\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdeepcopy\u001b[39m(x, memo\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, _nil\u001b[38;5;241m=\u001b[39m[]):\n\u001b[1;32m    129\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Deep copy operation on arbitrary Python objects.\u001b[39;00m\n\u001b[1;32m    130\u001b[0m \n\u001b[1;32m    131\u001b[0m \u001b[38;5;124;03m    See the module's __doc__ string for more info.\u001b[39;00m\n\u001b[1;32m    132\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m    134\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m memo \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import neuralprocesses.torch as nps\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "GRID_SIZE = 10\n",
    "AGENT_VIEW_SIZE = 3\n",
    "TRAJ_LENGTH = 10\n",
    "\n",
    "agnp = nps.construct_agnp(dim_x=2, dim_y=5, likelihood=\"het\", dim_lv = 256).to(device)\n",
    "#gnp = nps.construct_gnp(dim_x = 2, dim_y = 5, likelihood=\"het\").to(device)\n",
    "opt = torch.optim.Adam(agnp.parameters(), lr=1e-5)\n",
    "sampler = Sampler(grid_size = GRID_SIZE, agent_view_size = AGENT_VIEW_SIZE, traj_length = TRAJ_LENGTH)\n",
    "\n",
    "if training:\n",
    "    for i in range(50000):\n",
    "        #(samples, batch_size, dim, n_points)\n",
    "        xc, yc, xt, yt, _ = get_batch(sampler)\n",
    "\n",
    "        dist = agnp(xc, yc, xt)\n",
    "          \n",
    "        logits = dist.mean  \n",
    "        log_probs = F.log_softmax(logits, dim=-2)  # Apply log softmax to get log-probabilities\n",
    "        \n",
    "        nll = -(log_probs * yt).sum(dim=-2)  # Sum over the class dimension\n",
    "        loss = nll.mean()\n",
    "    \n",
    "        #Calculate the KL divergence by sampling????\n",
    "        kl = calc_kl_divergence(agnp, xc, yc, xt, yt)\n",
    "        \n",
    "        opt.zero_grad(set_to_none=True)\n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "        \n",
    "        if i % 100 == 0:\n",
    "            ##TODO: instead of accuracy, calculate the manhattan distance of the last position after the sequence\n",
    "            predicted = F.softmax(dist.mean, dim = -2).argmax(dim=-2)\n",
    "            targets = yt.argmax(dim=-2)\n",
    "            accuracy = (predicted == targets).float().mean()\n",
    "            traj_divergence = construct_and_calc_l1_dist(targets, predicted)\n",
    "            print(f\"Iteration {i}, Loss: {loss.item():.4f}, Accuracy: {accuracy.item():.4f} mean distance difference: {traj_divergence.mean().item():.4f}, Final distance difference: {traj_divergence[-1].item():.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "if training:\n",
    "  PATH = os.path.abspath(os.getcwd())\n",
    "  torch.save(agnp.state_dict(), PATH + \"/models/anp.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Model(\n",
       "    Chain(\n",
       "        Chain(\n",
       "            SqueezeParallel(),\n",
       "            AssertNoParallel(),\n",
       "        ),\n",
       "        Copy(),\n",
       "        Parallel(\n",
       "            Chain(\n",
       "                RepeatForAggregateInputs(\n",
       "                  (coder): InputsCoder()\n",
       "                ),\n",
       "                DeterministicLikelihood(),\n",
       "            ),\n",
       "            Parallel(\n",
       "                Chain(\n",
       "                    RepeatForAggregateInputs(\n",
       "                      (coder): Attention(\n",
       "                        (encoder_x): MLP(\n",
       "                          (net): Sequential(\n",
       "                            (0): Linear(in_features=2, out_features=256, bias=True)\n",
       "                            (1): ReLU()\n",
       "                            (2): Linear(in_features=256, out_features=256, bias=True)\n",
       "                            (3): ReLU()\n",
       "                            (4): Linear(in_features=256, out_features=256, bias=True)\n",
       "                            (5): ReLU()\n",
       "                            (6): Linear(in_features=256, out_features=256, bias=True)\n",
       "                          )\n",
       "                        )\n",
       "                        (encoder_xy): MLP(\n",
       "                          (net): Sequential(\n",
       "                            (0): Linear(in_features=7, out_features=256, bias=True)\n",
       "                            (1): ReLU()\n",
       "                            (2): Linear(in_features=256, out_features=256, bias=True)\n",
       "                            (3): ReLU()\n",
       "                            (4): Linear(in_features=256, out_features=256, bias=True)\n",
       "                            (5): ReLU()\n",
       "                            (6): Linear(in_features=256, out_features=256, bias=True)\n",
       "                          )\n",
       "                        )\n",
       "                        (mixer): MLP(\n",
       "                          (net): Linear(in_features=256, out_features=256, bias=True)\n",
       "                        )\n",
       "                        (mlp1): MLP(\n",
       "                          (net): Linear(in_features=256, out_features=256, bias=True)\n",
       "                        )\n",
       "                        (ln1): Sequential(\n",
       "                          (0): _LambdaModule()\n",
       "                          (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "                          (2): _LambdaModule()\n",
       "                        )\n",
       "                        (mlp2): MLP(\n",
       "                          (net): Sequential(\n",
       "                            (0): Linear(in_features=256, out_features=256, bias=True)\n",
       "                            (1): ReLU()\n",
       "                            (2): Linear(in_features=256, out_features=256, bias=True)\n",
       "                          )\n",
       "                        )\n",
       "                        (ln2): Sequential(\n",
       "                          (0): _LambdaModule()\n",
       "                          (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "                          (2): _LambdaModule()\n",
       "                        )\n",
       "                      )\n",
       "                    ),\n",
       "                    DeterministicLikelihood(),\n",
       "                ),\n",
       "            ),\n",
       "        ),\n",
       "    ),\n",
       "    Chain(\n",
       "        Concatenate(),\n",
       "        RepeatForAggregateInputs(\n",
       "          (coder): Chain(\n",
       "              MLP(\n",
       "                (net): Sequential(\n",
       "                  (0): Linear(in_features=258, out_features=512, bias=True)\n",
       "                  (1): ReLU()\n",
       "                  (2): Linear(in_features=512, out_features=512, bias=True)\n",
       "                  (3): ReLU()\n",
       "                  (4): Linear(in_features=512, out_features=512, bias=True)\n",
       "                  (5): ReLU()\n",
       "                  (6): Linear(in_features=512, out_features=512, bias=True)\n",
       "                  (7): ReLU()\n",
       "                  (8): Linear(in_features=512, out_features=512, bias=True)\n",
       "                  (9): ReLU()\n",
       "                  (10): Linear(in_features=512, out_features=512, bias=True)\n",
       "                  (11): ReLU()\n",
       "                  (12): Linear(in_features=512, out_features=10, bias=True)\n",
       "                )\n",
       "              ),\n",
       "              SelectFromChannels(),\n",
       "          )\n",
       "        ),\n",
       "        HeterogeneousGaussianLikelihood(epsilon=1e-06),\n",
       "        _LambdaModule(),\n",
       "    )\n",
       ")"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "if not training:\n",
    "  agnp = nps.construct_agnp(dim_x = 2, dim_y = 4, likelihood = \"het\").to(device)\n",
    "  agnp.load_state_dict(torch.load('models/anp.pth'))\n",
    "\n",
    "agnp.eval().to(\"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make predictions on new trajectories, and compare"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "agnp.eval().to(\"cpu\")\n",
    "xc, yc, xt, yt, _ = get_batch(grid_size = 10, agent_view_size = 3, traj_length = 7, device = device)\n",
    "dist = agnp(xc, yc, xt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[[0.0188],\n",
       "          [0.0234],\n",
       "          [0.1705],\n",
       "          [0.0986],\n",
       "          [0.6886]],\n",
       " \n",
       "         [[0.0190],\n",
       "          [0.0236],\n",
       "          [0.1710],\n",
       "          [0.0991],\n",
       "          [0.6872]],\n",
       " \n",
       "         [[0.0196],\n",
       "          [0.0243],\n",
       "          [0.1725],\n",
       "          [0.1004],\n",
       "          [0.6833]],\n",
       " \n",
       "         [[0.0188],\n",
       "          [0.0234],\n",
       "          [0.1705],\n",
       "          [0.0986],\n",
       "          [0.6886]],\n",
       " \n",
       "         [[0.0188],\n",
       "          [0.0234],\n",
       "          [0.1705],\n",
       "          [0.0986],\n",
       "          [0.6886]]], grad_fn=<SoftmaxBackward0>),\n",
       " tensor([[[0.],\n",
       "          [0.],\n",
       "          [0.],\n",
       "          [0.],\n",
       "          [1.]],\n",
       " \n",
       "         [[0.],\n",
       "          [0.],\n",
       "          [0.],\n",
       "          [1.],\n",
       "          [0.]],\n",
       " \n",
       "         [[0.],\n",
       "          [0.],\n",
       "          [0.],\n",
       "          [1.],\n",
       "          [0.]],\n",
       " \n",
       "         [[0.],\n",
       "          [0.],\n",
       "          [0.],\n",
       "          [0.],\n",
       "          [1.]],\n",
       " \n",
       "         [[0.],\n",
       "          [0.],\n",
       "          [0.],\n",
       "          [0.],\n",
       "          [1.]]]))"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "F.softmax(dist.mean, dim=-2), yt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'mean' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[97], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mmean\u001b[49m[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39margmax(), yt[\u001b[38;5;241m0\u001b[39m]\n",
      "\u001b[0;31mNameError\u001b[0m: name 'mean' is not defined"
     ]
    }
   ],
   "source": [
    "mean[0].argmax(), yt[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The following is a copy from the relational_neural_process githu\n",
    "\n",
    "Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import numpy.random as npr\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import collections\n",
    "import matplotlib.pyplot as plt\n",
    "import datetime\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class CNPDeterministicEncoder(nn.Module):\n",
    "    def __init__(self, sizes):\n",
    "        super(CNPDeterministicEncoder, self).__init__()\n",
    "        self.linears = nn.ModuleList()\n",
    "        for i in range(len(sizes) - 1):\n",
    "            self.linears.append(nn.Linear(sizes[i], sizes[i + 1]))\n",
    "\n",
    "    def forward(self, context_x, context_y):\n",
    "        \"\"\"\n",
    "        Encode training set as one vector representation\n",
    "\n",
    "        Args:\n",
    "            context_x: batch_size x set_size x feature_dim_x\n",
    "            context_y: batch_size x set_size x feature_dim_y\n",
    "\n",
    "        Returns: representation: batch_size x representation_size:\n",
    "        \"\"\"\n",
    "\n",
    "        encoder_input = torch.cat((context_x, context_y), dim = -1)\n",
    "        batch_size, set_size, filter_size = encoder_input.shape\n",
    "        x = encoder_input.view(batch_size * set_size, -1)\n",
    "        for i, linear in enumerate(self.linears[:-1]):\n",
    "            x = torch.relu(linear(x))\n",
    "        x = self.linears[-1](x)\n",
    "        x = x.view(batch_size, set_size, -1)\n",
    "        representation = x.sum(dim=1)\n",
    "        return representation\n",
    "            \n",
    "class CNPDeterministicDecoder(nn.Module):\n",
    "    def __init__(self, sizes):\n",
    "        super(CNPDeterministicDecoder, self).__init__()\n",
    "        self.linears = nn.ModuleList()\n",
    "        for i in range(len(sizes) - 1):\n",
    "            self.linears.append(nn.Linear(sizes[i], sizes[i + 1]))\n",
    "\n",
    "    def forward(self, representation, target_x):\n",
    "        \"\"\"\n",
    "        Take representation representation of current training set, and a target input x,\n",
    "        return the predictive distribution at x (Gaussian with mean mu and scale sigma)\n",
    "\n",
    "        Args:\n",
    "            representation: batch_size x representation_size\n",
    "            target_x: batch_size x set_size x d\n",
    "        \"\"\"\n",
    "        batch_size, set_size, d = target_x.shape\n",
    "        \n",
    "        if representation is None:        \n",
    "            input = target_x            \n",
    "        else:\n",
    "            representation = representation.unsqueeze(1).repeat([1, set_size, 1])\n",
    "            input = torch.cat((representation, target_x), dim=-1)\n",
    "        \n",
    "        #All rows\n",
    "        x = input.view(batch_size * set_size, -1)\n",
    "        for linear in self.linears[:-1]:\n",
    "            x = torch.relu(linear(x))\n",
    "        logits = self.linears[-1](x)\n",
    "        logits = logits.view(batch_size, set_size, -1)\n",
    "        probs = F.softmax(logits, dim = -1)\n",
    "\n",
    "        dist = torch.distributions.categorical.Categorical(probs = probs)\n",
    "        return dist, probs, logits\n",
    "    \n",
    "        '''\n",
    "        mu, log_sigma = torch.split(out, 1, dim = -1)\n",
    "        sigma = 0.01 + 0.99 * torch.nn.functional.softplus(log_sigma)\n",
    "        dist = torch.distributions.normal.Normal(loc=mu, scale=sigma)\n",
    "        '''\n",
    "\n",
    "class CNPDeterministicModel(nn.Module):\n",
    "    def __init__(self, encoder_size, decoder_size):\n",
    "        super(CNPDeterministicModel, self).__init__()\n",
    "        self._encoder = CNPDeterministicEncoder(encoder_size)\n",
    "        self._decoder = CNPDeterministicDecoder(decoder_size)\n",
    "\n",
    "\n",
    "    def forward(self, query, target_y = None):\n",
    "        (context_x, context_y), target_x = query\n",
    "        representation = self._encoder(context_x, context_y)\n",
    "        dist, probs, logits = self._decoder(representation, target_x)\n",
    "\n",
    "        log_p = None\n",
    "        if target_y is not None:\n",
    "            #Reverse one hot encoding on target_y\n",
    "            target_y = torch.argmax(target_y, dim = -1)\n",
    "            log_p = dist.log_prob(target_y)\n",
    "\n",
    "        return log_p, probs, logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CNPDeterministicModel(\n",
       "  (_encoder): CNPDeterministicEncoder(\n",
       "    (linears): ModuleList(\n",
       "      (0): Linear(in_features=6, out_features=128, bias=True)\n",
       "      (1-2): 2 x Linear(in_features=128, out_features=128, bias=True)\n",
       "      (3): Linear(in_features=128, out_features=258, bias=True)\n",
       "    )\n",
       "  )\n",
       "  (_decoder): CNPDeterministicDecoder(\n",
       "    (linears): ModuleList(\n",
       "      (0): Linear(in_features=260, out_features=128, bias=True)\n",
       "      (1-2): 2 x Linear(in_features=128, out_features=128, bias=True)\n",
       "      (3): Linear(in_features=128, out_features=4, bias=True)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import neuralprocesses.torch as nps\n",
    "\n",
    "d_x, d_in, representation_size, d_out, hidden_size = 2, 6, 258, 4, 128\n",
    "encoder_sizes = [d_in, hidden_size, hidden_size, hidden_size, representation_size]\n",
    "decoder_sizes = [representation_size + d_x, hidden_size, hidden_size, hidden_size, d_out]\n",
    "\n",
    "model = CNPDeterministicModel(encoder_size=encoder_sizes, decoder_size=decoder_sizes)\n",
    "model.to(device)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pml2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
